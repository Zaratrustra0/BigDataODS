# 图解机器学习 | 决策树模型详解
引言
--

**决策树**（Decision Tree）是机器学习中一种经典的分类与回归算法。在本篇中我们讨论用于分类的决策树的原理知识。决策树模型呈树形结构，在分类问题中，一颗决策树可以视作 if-then 规则的集合。模型具有可读性，分类速度快的特点，在各种实际业务建模过程中广泛使用。

（本篇内容会涉及到不少机器学习基础知识，没有先序知识储备的宝宝可以查看ShowMeAI的文章 [图解机器学习 | 机器学习基础知识](https://www.showmeai.tech/article-detail/185)。

1.决策树算法核心思想
-----------

### 1）决策树结构与核心思想

决策树（Decision tree）是基于已知各种情况（特征取值）的基础上，通过构建树型决策结构来进行分析的一种方式，是常用的有监督的分类算法。

**决策树模型（Decision Tree model）模拟人类决策过程**。以买衣服为例，一个顾客在商店买裤子，于是有了下面的对话：

![](https://img-blog.csdnimg.cn/img_convert/0ae3c195e46617040f9961f601f3fa20.png)

决策树是一种预测模型，代表的是对象属性与对象值之间的映射关系。决策树是一种树形结构，其中：

*   每个内部结点表示一个属性的测试
*   每个分支表示一个测试输出
*   每个叶结点代表一种类别

![](https://img-blog.csdnimg.cn/img_convert/b82ec52f0560f9803e7855c3e5267a47.png)

> 如上图买衣服的例子，第一个「**内部结点**」对应于属性「材料」上的测试，两个分支分别是该属性取值为「牛仔」和「非牛仔」两种可能结果。当取值为「牛仔」时，则对下个属性「裤型」进行测试；若取值为「非牛仔」时，则对应于「叶结点」——「不买」。

**决策树模型核心**是下面几部分：

*   结点和有向边组成。
*   结点有内部结点和叶结点俩种类型。
*   内部结点表示一个特征，叶结点表示一个类。

![](https://img-blog.csdnimg.cn/img_convert/21d9942f26e5acd5e50f7443258f7e41.png)

### 2）决策树的发展史

决策树在发展过程中，有过很多不同类型的模型，典型的模型如ID3、C4.5和CART等，下面我们来简单介绍一下发展史中不同的模型。

![](https://img-blog.csdnimg.cn/img_convert/26d59fcd1c56f7b53c76e050db98712d.png)

2.决策树生长与最优属性的选择
---------------

上面介绍的决策树发展史里，大家对于不同的决策树模型有一个基础的理解了，下面一部分，我们来一起看一下决策树是如何生长构成的。

### 1）决策树生长流程

决策树的决策过程就是从根结点开始，测试待分类项中对应的特征属性，并按照其值选择输出分支，直到叶子结点，将叶子结点的存放的类别作为决策结果。**简单说来，决策树的总体流程是自根至叶的递归过程，在每个中间结点寻找一个「划分」（split or test）属性**。

如下图的伪代码，是详细的决策树生长（构建）流程。大家可以特别注意图中3类终止条件和返回的结果，而整个流程中，有非常核心的一步是「**最优划分属性的选择**」。

![](https://img-blog.csdnimg.cn/img_convert/6f9a17ac9149ab1792b269c8f6cae354.png)

决策树停止生长的三个条件：

![](https://img-blog.csdnimg.cn/img_convert/6d01eee123bb3e9fc1ece2a262fb54ff.png)

### 2）最优属性选择

下面我们来看看，决策树的最优划分属性选择，是怎么做的。

#### （1）信息熵

要了解决策树的「最优属性」选择，我们需要先了解一个信息论的概念「**信息熵（entropy）**」（相关知识可以参考ShowMeAI文章 [**图解AI数学基础 | 信息论**](https://www.showmeai.tech/article-detail/164)），它是消除不确定性所需信息量的度量，也是未知事件可能含有的信息量，可以度量样本集合「纯度」。

对应到机器学习中，假定当前数据集 ![](https://www.zhihu.com/equation?tex=D)
 中有 ![](https://www.zhihu.com/equation?tex=y)
 类，其中第 ![](https://www.zhihu.com/equation?tex=k)
 类样本占比为 ![](https://www.zhihu.com/equation?tex=p_%7Bk%7D)
，则信息熵的计算公式如下：

![](https://www.zhihu.com/equation?tex=Ent%28D%29%20%3D%20-%5Csum_%7BK%3D1%7D%5E%7B%5Cleft%20%7C%20y%20%5Cright%20%7C%20%7D%20p_%7Bk%7D%20%5Clog_%7B2%7D%7Bp_%7Bk%7D%7D)

![](https://img-blog.csdnimg.cn/img_convert/77d19c99b9d4014a712515c7fa57a213.png)

但 ![](https://www.zhihu.com/equation?tex=p_k)
 取值为 ![](https://www.zhihu.com/equation?tex=1)
 的时候，信息熵为 ![](https://www.zhihu.com/equation?tex=0)
（很显然这时候概率 ![](https://www.zhihu.com/equation?tex=1)
 表示确定事件，没有任何不确定性）；而当 ![](https://www.zhihu.com/equation?tex=p_k)
 是均匀分布的时候，信息熵取最大值 ![](https://www.zhihu.com/equation?tex=%5Clog%28%7Cy%7C%29)
（此时所有候选同等概率，不确定性最大）。

#### （2）信息增益

大家对信息熵有了解后，我们就可以进一步了解信息增益（Information Gain），它衡量的是我们**选择某个属性进行划分时信息熵的变化**（可以理解为基于这个规则划分，不确定性降低的程度）。

![](https://www.zhihu.com/equation?tex=%5Coperatorname%7BGain%7D%28D%2C%20a%29%3D%5Coperatorname%7BEnt%7D%28D%29-%5Csum_%7Bv%3D1%7D%5E%7Bv%7D%20%5Cfrac%7B%5Cleft%7CD%5E%7Bv%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Coperatorname%7BEnt%7D%5Cleft%28D%5E%7Bv%7D%5Cright%29)

![](https://img-blog.csdnimg.cn/img_convert/0d4a8e0f14c1a99e9202a37af24e59de.png)

信息增益描述了一个特征带来的信息量的多少。在决策树分类问题中，信息增益就是决策树在进行属性选择划分前和划分后的信息差值。典型的决策树算法ID3就是基于信息增益来挑选每一节点分支用于划分的属性（特征）的。

这里以西瓜数据集为例。

将数据带入信息熵公式，即可得到根结点的信息熵。

![](https://img-blog.csdnimg.cn/img_convert/813278091275dac0b376dc1d78bc1975.png)

以属性「色泽」为例，其对应的 ![](https://www.zhihu.com/equation?tex=3)
 个数据子集：

![](https://img-blog.csdnimg.cn/img_convert/60abef5910384e03a4e9d463d9f89680.png)

**色泽属性的信息增益为**：

![](https://img-blog.csdnimg.cn/img_convert/3244af695f6318d8dfea258aa59f39d9.png)

同样的方法，计算**其他属性的信息增益为**：

![](https://img-blog.csdnimg.cn/img_convert/2163099f3b4ae8eebaa27510b5aa7121.png)

对比不同属性，我们发现「**纹理**」信息增益最大，其被选为划分属性：清晰 ![](https://www.zhihu.com/equation?tex=%5Cleft%20%5C%7B%201%2C2%2C3%2C4%2C5%2C6%2C8%2C10%2C15%20%20%5Cright%20%5C%7D)
、稍糊 ![](https://www.zhihu.com/equation?tex=%5Cleft%20%5C%7B%207%2C9%2C13%2C14%2C17%20%20%5Cright%20%5C%7D)
、模糊 ![](https://www.zhihu.com/equation?tex=%5Cleft%20%5C%7B%20%2011%2C12%2C16%20%5Cright%20%5C%7D)
。

再往下一步，我们看看「纹理」=「清晰」的节点分支，该节点包含的样例集合 ![](https://www.zhihu.com/equation?tex=D1)
 中有编号为 ![](https://www.zhihu.com/equation?tex=%5Cleft%20%5C%7B%201%2C2%2C3%2C4%2C5%2C6%2C8%2C10%2C15%20%20%5Cright%20%5C%7D)
 共计 ![](https://www.zhihu.com/equation?tex=9)
 个样例，可用属性集合为 ![](https://www.zhihu.com/equation?tex=%5Cleft%20%5C%7B%20%E8%89%B2%E6%B3%BD%2C%E6%A0%B9%E8%92%82%2C%E6%95%B2%E5%A3%B0%2C%E8%84%90%E9%83%A8%2C%E8%A7%A6%E6%84%9F%20%20%5Cright%20%5C%7D)
（此时「纹理」不再作为划分属性），我们同样的方式再计算各属性的信息增益为：

![](https://img-blog.csdnimg.cn/img_convert/a30b0ca76617130396324a17b302343e.png)

从上图可以看出「根蒂」、「脐部」、「触感」![](https://www.zhihu.com/equation?tex=3)
 个属性均取得了最大的信息增益，可用任选其一作为划分属性。同理，对每个分支结点进行类似操作，即可得到最终的决策树。

![](https://img-blog.csdnimg.cn/img_convert/4653f4d741710266b885563b08643cf4.png)

#### （3）信息增益率（Gain Ratio）

大家已经了解了信息增益作为特征选择的方法，但信息增益有一个问题，它偏向取值较多的特征。原因是，当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的。因此信息增益更大，因此信息增益比较偏向取值较多的特征。

那有没有解决这个小问题的方法呢？有的，这就是我们要提到信息增益率（Gain Ratio），信息增益率相比信息增益，多了一个衡量本身属性的分散程度的部分作为分母，而著名的决策树算法C4.5就是使用它作为划分属性挑选的原则。

**信息增益率的计算细节**如下所示：

![](https://www.zhihu.com/equation?tex=%5Coperatorname%7BGain%7D_%7B-%7D%20%5Coperatorname%7Bratio%7D%28D%2C%20a%29%3D%5Cfrac%7B%5Coperatorname%7BGain%7D%28D%2C%20a%29%7D%7B%5Coperatorname%7BIV%7D%28a%29%7D)

  

![](https://www.zhihu.com/equation?tex=IV%28a%29%3D-%5Csum_%7Bv%3D1%7D%5E%7BV%7D%20%5Cfrac%7B%5Cleft%7CD%5E%7Bv%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CD%5E%7Bv%7D%5Cright%7C%7D%7B%7CD%7C%7D)

![](https://img-blog.csdnimg.cn/img_convert/193df992f3a5c4728c8b809b639b44db.png)

数学上用于信息量（或者纯度）衡量的不止有上述的熵相关的定义，我们还可以使用基尼指数来表示数据集的不纯度。基尼指数越大，表示数据集越不纯。

**基尼指数**（Gini Index）的详细计算方式如下所示：

![](https://www.zhihu.com/equation?tex=%5Coperatorname%7BGini%7D%28D%29%3D%5Csum_%7Bk%3D1%7D%5E%7B%7Cy%7C%7D%20%5Csum_%7Bk%20%5Cprime%20%5Cneq%20k%7D%20p_%7Bk%7D%20p_%7Bk%5E%7B%5Cprime%7D%7D%3D1-%5Csum_%7Bk%3D1%7D%5E%7B%7Cy%7C%7D%20p_%7Bk%7D%5E%7B2%7D)

![](https://img-blog.csdnimg.cn/img_convert/d33ab1b9a06232c6ee51a198b787ffb5.png)

其中，![](https://www.zhihu.com/equation?tex=p_k)
 表示第 ![](https://www.zhihu.com/equation?tex=k)
 类的数据占总数据的比例，著名的决策树算法CART就是使用基尼指数来进行划分属性的挑选（当然，CART本身是二叉树结构，这一点和上述的 ID3 和 C4.5 不太一样）。

对于基尼指数的一种理解方式是，之所以它可以用作纯度的度量，大家可以想象在一个漆黑的袋里摸球，有不同颜色的球，其中第k类占比记作 ![](https://www.zhihu.com/equation?tex=p_k)
，那两次摸到的球都是第k类的概率就是 ![](https://www.zhihu.com/equation?tex=p_k%5E2)
，那两次摸到的球颜色不一致的概率就是 ![](https://www.zhihu.com/equation?tex=1-%5Csum%20p_k%5E2)
 ，它的取值越小，两次摸球颜色不一致的概率就越小，纯度就越高。

3.过拟合与剪枝
--------

如果我们让决策树一直生长，最后得到的决策树可能很庞大，而且因为对原始数据学习得过于充分会有过拟合的问题。缓解决策树过拟合可以通过剪枝操作完成。而剪枝方式又可以分为：预剪枝和后剪枝。

### 1）决策树与剪枝操作

为了尽可能正确分类训练样本，有可能造成分支过多，造成过拟合。过拟合是指训练集上表现很好，但是在测试集上表现很差，泛化性能差。**可以通过剪枝主动去掉一些分支来降低过拟合的风险，并使用「留出法」进行评估剪枝前后决策树的优劣**。

基本策略包含「**预剪枝**」和「**后剪枝**」两个：

*   **预剪枝**（**pre-pruning**）：在决策树生长过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶结点。
    
*   **后剪枝**（**post-pruning**）：先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。
    

### 2）预剪枝与后剪枝案例

我们来看一个例子，下面的数据集，为了评价决策树模型的表现，会划分出一部分数据作为验证集。

![](https://img-blog.csdnimg.cn/img_convert/906b55140cb74db4483ac292920ba3f7.png)

在上述西瓜数据集上生成的一颗完整的决策树，如下图所示。

![](https://img-blog.csdnimg.cn/img_convert/b2875f98c84e7e3dfb7edbf23b0aef39.png)

#### （1）预剪枝

「预剪枝」过程如下：将其标记为叶结点，类别标记为训练样例中最多的类别。

*   若选「**好瓜**」，验证集中 ![](https://www.zhihu.com/equation?tex=%5Cleft%20%5C%7B%204%2C5%2C8%20%5Cright%20%5C%7D)
     被分类正确，得到验证集精度为3/7x100%=42.9%
    
*   根据结点 ② ③ ④ 的训练样例，将这 ![](https://www.zhihu.com/equation?tex=3)
     个结点分别标记为「好瓜」、「好瓜」、「坏瓜」。此时，验证集中编号为 ![](https://www.zhihu.com/equation?tex=%5Cleft%20%5C%7B%204%2C5%2C8%2C11%2C12%20%5Cright%20%5C%7D)
     的样例被划分正确，验证集精度为 ![](https://www.zhihu.com/equation?tex=5/7x100%25%3D71.4%25)
    

![](https://img-blog.csdnimg.cn/img_convert/5fea16b3d617ec6629d3850e28b9a503.png)

若划分后的验证集精度下降，则拒绝划分。对结点 ② ③ ④ 分别进行剪枝判断，结点 ② ③ 都禁止划分，结点 ④ 本身为叶子结点。

根据预剪枝方法，此处生成了一层决策树。这种最终得到仅有一层划分的决策树，称为「决策树桩」（decision stump）。

![](https://img-blog.csdnimg.cn/img_convert/1448910dae3f5d192f9dee4fb60cd264.png)

#### （2）后剪枝

我们在生成的完整决策树上进行「后剪枝」：

剪枝后的精度提升了，因此该决策树需要在结点 ⑥ 处进行剪枝。

![](https://img-blog.csdnimg.cn/img_convert/a53757d58bb641652147e60ace6d7cfc.png)

考虑结点⑤，若将其替换为叶结点，根据落在其上的训练样例 ![](https://www.zhihu.com/equation?tex=%5Cleft%20%5C%7B%206%2C7%2C15%20%5Cright%20%5C%7D)
 将其标记为「好瓜」，测得验证集精度仍为 ![](https://www.zhihu.com/equation?tex=57.1%25)
，可以不剪枝。

![](https://img-blog.csdnimg.cn/img_convert/835545dffaecfbc4a8ce352735cd0e30.png)

考虑结点②，若将其替换为叶结点，根据落在其上的训练样例 ![](https://www.zhihu.com/equation?tex=%5Cleft%20%5C%7B%201%2C2%2C3%2C14%20%5Cright%20%5C%7D)
 将其标记为「好瓜」，测得验证集精度提升至 ![](https://www.zhihu.com/equation?tex=71.4%25)
，决定剪枝。

![](https://img-blog.csdnimg.cn/img_convert/33e57086253b3200d3153f1d6dc50882.png)

对结点 ③ 和 ①，若将其子树替换为叶结点，则所得决策树的验证集精度分布为 ![](https://www.zhihu.com/equation?tex=71.4%25)
 和 ![](https://www.zhihu.com/equation?tex=42.9%25)
，均未提高，所以不剪枝。得到最终后剪枝之后的决策树。

![](https://img-blog.csdnimg.cn/img_convert/d1ebcb7589710f37a542c48aa067ee63.png)

### 3）预剪枝与后剪枝的特点

**时间开销**：

*   预剪枝：训练时间开销降低，测试时间开销降低。
*   后剪枝：训练时间开销增加，测试时间开销降低。

**过/欠拟合风险**：

*   预剪枝：过拟合风险降低，欠拟合风险增加。
*   后剪枝：过拟合风险降低，欠拟合风险基本不变。

**泛化性能**：后剪枝通常优于预剪枝。

4.连续值与缺失值的处理
------------

### 1）连续值处理

我们用于学习的数据包含了连续值特征和离散值特征，之前的例子中使用的都是离散值属性（特征），决策树当然也能处理连续值属性，我们来看看它的处理方式。

对于**离散取值的特征**，决策树的划分方式是：选取一个最合适的特征属性，然后将集合按照这个特征属性的不同值划分为多个子集合，并且不断的重复这种操作的过程。

对于**连续值属性**，显然我们不能以这些离散值直接进行分散集合，否则每个连续值将会对应一种分类。那我们如何把连续值属性参与到决策树的建立中呢？

因为连续属性的可取值数目不再有限，因此需要连续属性离散化处理，**常用的离散化策略是二分法**，这个技术也是 C4.5 中采用的策略。

具体的二分法处理方式如下图所示：

![](https://img-blog.csdnimg.cn/img_convert/0c35b0eb6c0c67204d9f62896fc9de87.png)

注意：与离散属性不同，若当前结点划分属性为连续属性，该属性还可以作为其后代结点的划分属性。

### 2）缺失值处理

原始数据很多时候还会出现缺失值，决策树算法也能有效的处理含有缺失值的数据。使用决策树建模时，处理缺失值需要解决2个问题：

*   **Q1：如何进行划分属性选择**？
*   **Q2：给定划分属性，若样本在该属性上的值缺失，如何进行划分**？

缺失值处理的基本思路是：样本赋权，权重划分。我们来通过下图这份有缺失值的西瓜数据集，看看具体处理方式。

仅通过无缺失值的样例来判断划分属性的优劣，学习开始时，根结点包含样例集 ![](https://www.zhihu.com/equation?tex=D)
 中全部 ![](https://www.zhihu.com/equation?tex=17)
 个样例，权重均为 ![](https://www.zhihu.com/equation?tex=1)
。

将数据带入信息熵计算公式即可得到该结点的信息熵。

![](https://img-blog.csdnimg.cn/img_convert/4acf768a3ba638864592a40dd4a7ff6d.png)

令 ![](https://www.zhihu.com/equation?tex=%5Ctilde%7BD%5E1%7D)
、![](https://www.zhihu.com/equation?tex=%5Ctilde%7BD%5E2%7D)
、![](https://www.zhihu.com/equation?tex=%5Ctilde%7BD%5E3%7D)
 分别表示在属性「色泽」上取值为「青绿」「乌黑」以及「浅白」的样本子集：

![](https://img-blog.csdnimg.cn/img_convert/c761749fa7399345a81586df365fb642.png)

于是，样本集 ![](https://www.zhihu.com/equation?tex=D)
 上属性「色泽」的信息增益可以计算得出，![](https://www.zhihu.com/equation?tex=Gain%28D%2C%E7%BA%B9%E7%90%86%29%3D0.424)
 信息增益最大，选择「纹理」作为接下来的划分属性。

于是，样本集 ![](https://www.zhihu.com/equation?tex=D)
 上属性「色泽」的信息增益可以计算得出，![](https://www.zhihu.com/equation?tex=Gain%28D%2C%E7%BA%B9%E7%90%86%29%3D0.424)
 信息增益最大，选择「纹理」作为接下来的划分属性。

![](https://img-blog.csdnimg.cn/img_convert/d99d86fc5a26686dd08b4a42ef78e50f.png)

更多监督学习的算法模型总结可以查看 ShowMeAI 的文章 [AI知识技能速查 | **机器学习-监督学习**](https://www.showmeai.tech/article-detail/113)。

* * *

视频教程
----

**可以点击 [B站](https://www.bilibili.com/video/BV1y44y187wN?p=12) 查看视频的【双语字幕】版本**

> [https://www.bilibili.com/video/BV1y44y187wN?p=12](https://www.bilibili.com/video/BV1y44y187wN?p=12)

机器学习【算法】系列教程
------------

*   [图解机器学习 | 机器学习基础知识](https://www.showmeai.tech/article-detail/185)
*   [图解机器学习 | 模型评估方法与准则](https://www.showmeai.tech/article-detail/186)
*   [图解机器学习 | KNN算法及其应用](https://www.showmeai.tech/article-detail/187)
*   [图解机器学习 | 逻辑回归算法详解](https://www.showmeai.tech/article-detail/188)
*   [图解机器学习 | 朴素贝叶斯算法详解](https://www.showmeai.tech/article-detail/189)
*   [图解机器学习 | 决策树模型详解](https://www.showmeai.tech/article-detail/190)
*   [图解机器学习 | 随机森林分类模型详解](https://www.showmeai.tech/article-detail/191)
*   [图解机器学习 | 回归树模型详解](https://www.showmeai.tech/article-detail/192)
*   [图解机器学习 | GBDT模型详解](https://www.showmeai.tech/article-detail/193)
*   [图解机器学习 | XGBoost模型最全解析](https://www.showmeai.tech/article-detail/194)
*   [图解机器学习 | LightGBM模型详解](https://www.showmeai.tech/article-detail/195)
*   [图解机器学习 | 支持向量机模型详解](https://www.showmeai.tech/article-detail/196)
*   [图解机器学习 | 聚类算法详解](https://www.showmeai.tech/article-detail/197)
*   [图解机器学习 | PCA降维算法详解](https://www.showmeai.tech/article-detail/198)

机器学习【实战】系列教程
------------

*   [机器学习实战 | Python机器学习算法应用实践](https://www.showmeai.tech/article-detail/201)
*   [机器学习实战 | SKLearn入门与简单应用案例](https://www.showmeai.tech/article-detail/202)
*   [机器学习实战 | SKLearn最全应用指南](https://www.showmeai.tech/article-detail/203)
*   [机器学习实战 | XGBoost建模应用详解](https://www.showmeai.tech/article-detail/204)
*   [机器学习实战 | LightGBM建模应用详解](https://www.showmeai.tech/article-detail/205)
*   [机器学习实战 | Python机器学习综合项目-电商销量预估](https://www.showmeai.tech/article-detail/206)
*   [机器学习实战 | Python机器学习综合项目-电商销量预估<进阶方案>](https://www.showmeai.tech/article-detail/207)
*   [机器学习实战 | 机器学习特征工程最全解读](https://www.showmeai.tech/article-detail/208)
*   [机器学习实战 | 自动化特征工程工具Featuretools应用](https://www.showmeai.tech/article-detail/209)
*   [机器学习实战 | AutoML自动化机器学习建模](https://www.showmeai.tech/article-detail/210)

[ShowMeAI](https://www.showmeai.tech/) 系列教程推荐
---------------------------------------------

*   [大厂技术实现：推荐与广告计算解决方案](https://www.showmeai.tech/tutorials/50)
*   [大厂技术实现：计算机视觉解决方案](https://www.showmeai.tech/tutorials/51)
*   [大厂技术实现：自然语言处理行业解决方案](https://www.showmeai.tech/tutorials/52)
*   [图解Python编程：从入门到精通系列教程](https://www.showmeai.tech/tutorials/56)
*   [图解数据分析：从入门到精通系列教程](https://www.showmeai.tech/tutorials/33)
*   [图解AI数学基础：从入门到精通系列教程](https://www.showmeai.tech/tutorials/83)
*   [图解大数据技术：从入门到精通系列教程](https://www.showmeai.tech/tutorials/84)
*   [图解机器学习算法：从入门到精通系列教程](https://www.showmeai.tech/tutorials/34)
*   [机器学习实战：手把手教你玩转机器学习系列](https://www.showmeai.tech/tutorials/41)
*   [深度学习教程：吴恩达专项课程 · 全套笔记解读](https://www.showmeai.tech/tutorials/35)
*   [自然语言处理教程：斯坦福CS224n课程 · 课程带学与全套笔记解读](https://www.showmeai.tech/tutorials/36)