# 大话CNN经典模型：AlexNet
_—— 原文发布于本人的微信公众号 “大数据与人工智能 Lab”（BigdataAILab），欢迎关注。_

![](https://static.oschina.net/uploads/space/2018/0312/010626_frBK_876354.png)

2012 年，Alex Krizhevsky、Ilya Sutskever 在多伦多大学 Geoff Hinton 的实验室设计出了一个深层的卷积神经网络 AlexNet，夺得了 2012 年 ImageNet LSVRC 的冠军，且准确率远超第二名（top5 错误率为 15.3%，第二名为 26.2%），引起了很大的轰动。AlexNet 可以说是具有历史意义的一个网络结构，在此之前，深度学习已经沉寂了很长时间，自 2012 年 AlexNet 诞生之后，后面的 ImageNet 冠军都是用卷积神经网络（CNN）来做的，并且层次越来越深，使得 CNN 成为在图像识别分类的核心算法模型，带来了深度学习的大爆发。  
在本博客之前的文章中已经介绍过了卷积神经网络（CNN）的技术原理（[大话卷积神经网络](https://my.oschina.net/u/876354/blog/1620906)），也回顾过卷积神经网络（CNN）的三个重要特点（[大话 CNN 经典模型：LeNet](https://my.oschina.net/u/876354/blog/1632862)），有兴趣的同学可以打开链接重新回顾一下，在此就不再重复 CNN 基础知识的介绍了。下面将先介绍 AlexNet 的特点，然后再逐层分解解析 AlexNet 网络结构。

**一、AlexNet 模型的特点**  
AlexNet 之所以能够成功，跟这个模型设计的特点有关，主要有：

*   使用了非线性激活函数：ReLU
*   防止过拟合的方法：Dropout，数据扩充（Data augmentation）
*   其他：多 GPU 实现，LRN 归一化层的使用

**1、使用 ReLU 激活函数**  
传统的神经网络普遍使用 Sigmoid 或者 tanh 等非线性函数作为激励函数，然而它们容易出现梯度弥散或梯度饱和的情况。以 Sigmoid 函数为例，当输入的值非常大或者非常小的时候，这些神经元的梯度接近于 0（梯度饱和现象），如果输入的初始值很大的话，梯度在反向传播时因为需要乘上一个 Sigmoid 导数，会造成梯度越来越小，导致网络变的很难学习。（详见本公博客的文章：[深度学习中常用的激励函数](https://my.oschina.net/u/876354/blog/1624376)）。  
在 AlexNet 中，使用了 ReLU （Rectified Linear Units）激励函数，该函数的公式为：f (x)=max (0,x)，当输入信号 < 0 时，输出都是 0，当输入信号 > 0 时，输出等于输入，如下图所示：  
![](https://static.oschina.net/uploads/space/2018/0312/010811_ZbmA_876354.png)
   
使用 ReLU 替代 Sigmoid/tanh，由于 ReLU 是线性的，且导数始终为 1，计算量大大减少，收敛速度会比 Sigmoid/tanh 快很多，如下图所示：  
![](https://static.oschina.net/uploads/space/2018/0312/010818_RCcD_876354.png)
   
**2、数据扩充（Data augmentation）**  
有一种观点认为神经网络是靠数据喂出来的，如果能够增加训练数据，提供海量数据进行训练，则能够有效提升算法的准确率，因为这样可以避免过拟合，从而可以进一步增大、加深网络结构。而当训练数据有限时，可以通过一些变换从已有的训练数据集中生成一些新的数据，以快速地扩充训练数据。  
其中，最简单、通用的图像数据变形的方式：水平翻转图像，从原始图像中随机裁剪、平移变换，颜色、光照变换，如下图所示：  
![](https://static.oschina.net/uploads/space/2018/0312/010829_NEg7_876354.png)
   
AlexNet 在训练时，在数据扩充（data augmentation）这样处理：  
（1）随机裁剪，对 256×256 的图片进行随机裁剪到 224×224，然后进行水平翻转，相当于将样本数量增加了（（256-224）^2）×2=2048 倍；  
（2）测试的时候，对左上、右上、左下、右下、中间分别做了 5 次裁剪，然后翻转，共 10 个裁剪，之后对结果求平均。作者说，如果不做随机裁剪，大网络基本上都过拟合；  
（3）对 RGB 空间做 PCA（主成分分析），然后对主成分做一个（0, 0.1）的高斯扰动，也就是对颜色、光照作变换，结果使错误率又下降了 1%。

**3、重叠池化 (Overlapping Pooling)**  
一般的池化（Pooling）是不重叠的，池化区域的窗口大小与步长相同，如下图所示：  
![](https://static.oschina.net/uploads/space/2018/0312/010844_Zvv8_876354.png)
   
在 AlexNet 中使用的池化（Pooling）却是可重叠的，也就是说，在池化的时候，每次移动的步长小于池化的窗口长度。AlexNet 池化的大小为 3×3 的正方形，每次池化移动步长为 2，这样就会出现重叠。重叠池化可以避免过拟合，这个策略贡献了 0.3% 的 Top-5 错误率。

**4、局部归一化（Local Response Normalization，简称 LRN）**  
在神经生物学有一个概念叫做 “侧抑制”（lateral inhibitio），指的是被激活的神经元抑制相邻神经元。归一化（normalization）的目的是 “抑制”，局部归一化就是借鉴了 “侧抑制” 的思想来实现局部抑制，尤其当使用 ReLU 时这种 “侧抑制” 很管用，因为 ReLU 的响应结果是无界的（可以非常大），所以需要归一化。使用局部归一化的方案有助于增加泛化能力。  
LRN 的公式如下，核心思想就是利用临近的数据做归一化，这个策略贡献了 1.2% 的 Top-5 错误率。  
![](https://static.oschina.net/uploads/space/2018/0312/010857_080B_876354.png)
   
**5、Dropout**  
引入 Dropout 主要是为了防止过拟合。在神经网络中 Dropout 通过修改神经网络本身结构来实现，对于某一层的神经元，通过定义的概率将神经元置为 0，这个神经元就不参与前向和后向传播，就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按照神经网络的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为 0），直至训练结束。  
Dropout 应该算是 AlexNet 中一个很大的创新，以至于 “神经网络之父” Hinton 在后来很长一段时间里的演讲中都拿 Dropout 说事。Dropout 也可以看成是一种模型组合，每次生成的网络结构都不一样，通过组合多个模型的方式能够有效地减少过拟合，Dropout 只需要两倍的训练时间即可实现模型组合（类似取平均）的效果，非常高效。  
如下图所示：  
![](https://static.oschina.net/uploads/space/2018/0312/010910_CExD_876354.png)
   
**6、多 GPU 训练**  
AlexNet 当时使用了 GTX580 的 GPU 进行训练，由于单个 GTX 580 GPU 只有 3GB 内存，这限制了在其上训练的网络的最大规模，因此他们在每个 GPU 中放置一半核（或神经元），将网络分布在两个 GPU 上进行并行计算，大大加快了 AlexNet 的训练速度。

**二、AlexNet 网络结构的逐层解析**  
下图是 AlexNet 的网络结构图：  
![](https://static.oschina.net/uploads/space/2018/0312/010926_sUJd_876354.png)
   
AlexNet 网络结构共有 8 层，前面 5 层是卷积层，后面 3 层是全连接层，最后一个全连接层的输出传递给一个 1000 路的 softmax 层，对应 1000 个类标签的分布。  
由于 AlexNet 采用了两个 GPU 进行训练，因此，该网络结构图由上下两部分组成，一个 GPU 运行图上方的层，另一个运行图下方的层，两个 GPU 只在特定的层通信。例如第二、四、五层卷积层的核只和同一个 GPU 上的前一层的核特征图相连，第三层卷积层和第二层所有的核特征图相连接，全连接层中的神经元和前一层中的所有神经元相连接。

下面逐层解析 AlexNet 结构：  
**1、第一层（卷积层）**  
![](https://static.oschina.net/uploads/space/2018/0312/010937_tT0h_876354.png)
   
该层的处理流程为：卷积 -->ReLU--> 池化 --> 归一化，流程图如下：  
![](https://static.oschina.net/uploads/space/2018/0312/010945_vBqi_876354.png)
   
**（1）卷积**  
输入的原始图像大小为 224×224×3（RGB 图像），在训练时会经过预处理变为 227×227×3。在本层使用 96 个 11×11×3 的卷积核进行卷积计算，生成新的像素。由于采用了两个 GPU 并行运算，因此，网络结构图中上下两部分分别承担了 48 个卷积核的运算。  
卷积核沿图像按一定的步长往 x 轴方向、y 轴方向移动计算卷积，然后生成新的特征图，其大小为：floor ((img\_size - filter\_size)/stride) +1 = new\_feture\_size，其中 floor 表示向下取整，img\_size 为图像大小，filter\_size 为核大小，stride 为步长，new\_feture\_size 为卷积后的特征图大小，这个公式表示图像尺寸减去卷积核尺寸除以步长，再加上被减去的核大小像素对应生成的一个像素，结果就是卷积后特征图的大小。  
AlexNet 中本层的卷积移动步长是 4 个像素，卷积核经移动计算后生成的特征图大小为 (227-11)/4+1=55，即 55×55。  
**（2）ReLU**  
卷积后的 55×55 像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为 2 组 55×55×48 的像素层数据。  
**（3）池化**  
RuLU 后的像素层再经过池化运算，池化运算的尺寸为 3×3，步长为 2，则池化后图像的尺寸为 (55-3)/2+1=27，即池化后像素的规模为 27×27×96  
**（4）归一化**  
池化后的像素层再进行归一化处理，归一化运算的尺寸为 5×5，归一化后的像素规模不变，仍为 27×27×96，这 96 层像素层被分为两组，每组 48 个像素层，分别在一个独立的 GPU 上进行运算。

**2、第二层（卷积层）**  
![](https://static.oschina.net/uploads/space/2018/0312/011009_V2kO_876354.png)
   
该层与第一层类似，处理流程为：卷积 -->ReLU--> 池化 --> 归一化，流程图如下：  
![](https://static.oschina.net/uploads/space/2018/0312/011018_OB4i_876354.png)
   
**（1）卷积**  
第二层的输入数据为第一层输出的 27×27×96 的像素层（被分成两组 27×27×48 的像素层放在两个不同 GPU 中进行运算），为方便后续处理，在这里每幅像素层的上下左右边缘都被填充了 2 个像素（填充 0），即图像的大小变为 (27+2+2) ×(27+2+2)。第二层的卷积核大小为 5×5，移动步长为 1 个像素，跟第一层第（1）点的计算公式一样，经卷积核计算后的像素层大小变为 (27+2+2-5)/1+1=27，即卷积后大小为 27×27。  
本层使用了 256 个 5×5×48 的卷积核，同样也是被分成两组，每组为 128 个，分给两个 GPU 进行卷积运算，结果生成两组 27×27×128 个卷积后的像素层。  
**（2）ReLU**  
这些像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为两组 27×27×128 的像素层。  
**（3）池化**  
再经过池化运算的处理，池化运算的尺寸为 3×3，步长为 2，池化后图像的尺寸为 (57-3)/2+1=13，即池化后像素的规模为 2 组 13×13×128 的像素层  
**（4）归一化**  
然后再经归一化处理，归一化运算的尺度为 5×5，归一化后的像素层的规模为 2 组 13×13×128 的像素层，分别由 2 个 GPU 进行运算。

**3、第三层（卷积层）**  
![](https://static.oschina.net/uploads/space/2018/0312/011038_357B_876354.png)
   
第三层的处理流程为：卷积 -->ReLU  
![](https://static.oschina.net/uploads/space/2018/0312/011044_HZub_876354.png)
   
**（1）卷积**  
第三层输入数据为第二层输出的 2 组 13×13×128 的像素层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后变为 (13+1+1)×(13+1+1)×128，分布在两个 GPU 中进行运算。  
这一层中每个 GPU 都有 192 个卷积核，每个卷积核的尺寸是 3×3×256。因此，每个 GPU 中的卷积核都能对 2 组 13×13×128 的像素层的所有数据进行卷积运算。如该层的结构图所示，两个 GPU 有通过交叉的虚线连接，也就是说每个 GPU 要处理来自前一层的所有 GPU 的输入。  
本层卷积的步长是 1 个像素，经过卷积运算后的尺寸为 (13+1+1-3)/1+1=13，即每个 GPU 中共 13×13×192 个卷积核，2 个 GPU 中共有 13×13×384 个卷积后的像素层。  
**（2）ReLU**  
卷积后的像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为 2 组 13×13×192 的像素层，分配给两组 GPU 处理。

**4、第四层（卷积层）**  
![](https://static.oschina.net/uploads/space/2018/0312/011103_CkAY_876354.png)
   
与第三层类似，第四层的处理流程为：卷积 -->ReLU  
 ![](https://static.oschina.net/uploads/space/2018/0312/011111_cpR3_876354.png)
  
**（1）卷积**  
第四层输入数据为第三层输出的 2 组 13×13×192 的像素层，类似于第三层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后的尺寸变为 (13+1+1)×(13+1+1)×192，分布在两个 GPU 中进行运算。  
这一层中每个 GPU 都有 192 个卷积核，每个卷积核的尺寸是 3×3×192（与第三层不同，第四层的 GPU 之间没有虚线连接，也即 GPU 之间没有通信）。卷积的移动步长是 1 个像素，经卷积运算后的尺寸为 (13+1+1-3)/1+1=13，每个 GPU 中有 13×13×192 个卷积核，2 个 GPU 卷积后生成 13×13×384 的像素层。  
**（2）ReLU**  
卷积后的像素层经过 ReLU 单元处理，生成激活像素层，尺寸仍为 2 组 13×13×192 像素层，分配给两个 GPU 处理。

**5、第五层（卷积层）**  
![](https://static.oschina.net/uploads/space/2018/0312/011126_0TlE_876354.png)
   
第五层的处理流程为：卷积 -->ReLU--> 池化  
![](https://static.oschina.net/uploads/space/2018/0312/011133_SPiu_876354.png)
   
**（1）卷积**  
第五层输入数据为第四层输出的 2 组 13×13×192 的像素层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后的尺寸变为 (13+1+1)×(13+1+1) ，2 组像素层数据被送至 2 个不同的 GPU 中进行运算。  
这一层中每个 GPU 都有 128 个卷积核，每个卷积核的尺寸是 3×3×192，卷积的步长是 1 个像素，经卷积后的尺寸为 (13+1+1-3)/1+1=13，每个 GPU 中有 13×13×128 个卷积核，2 个 GPU 卷积后生成 13×13×256 的像素层。  
**（2）ReLU**  
卷积后的像素层经过 ReLU 单元处理，生成激活像素层，尺寸仍为 2 组 13×13×128 像素层，由两个 GPU 分别处理。  
**（3）池化**  
2 组 13×13×128 像素层分别在 2 个不同 GPU 中进行池化运算处理，池化运算的尺寸为 3×3，步长为 2，池化后图像的尺寸为 (13-3)/2+1=6，即池化后像素的规模为两组 6×6×128 的像素层数据，共有 6×6×256 的像素层数据。

**6、第六层（全连接层）**  
![](https://static.oschina.net/uploads/space/2018/0312/011153_r5E1_876354.png)
   
第六层的处理流程为：卷积（全连接）-->ReLU-->Dropout  
![](https://static.oschina.net/uploads/space/2018/0312/011202_0hsF_876354.png)
   
**（1）卷积（全连接）**  
第六层输入数据是第五层的输出，尺寸为 6×6×256。本层共有 4096 个卷积核，每个卷积核的尺寸为 6×6×256，由于卷积核的尺寸刚好与待处理特征图（输入）的尺寸相同，即卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘，一一对应，因此，该层被称为全连接层。由于卷积核与特征图的尺寸相同，卷积运算后只有一个值，因此，卷积后的像素层尺寸为 4096×1×1，即有 4096 个神经元。  
**（2）ReLU**  
这 4096 个运算结果通过 ReLU 激活函数生成 4096 个值。  
**（3）Dropout**  
然后再通过 Dropout 运算，输出 4096 个结果值。

**7、第七层（全连接层）**  
![](https://static.oschina.net/uploads/space/2018/0312/011217_vDS8_876354.png)
   
第七层的处理流程为：全连接 -->ReLU-->Dropout  
![](https://static.oschina.net/uploads/space/2018/0312/011228_Vw8u_876354.png)
   
第六层输出的 4096 个数据与第七层的 4096 个神经元进行全连接，然后经 ReLU 进行处理后生成 4096 个数据，再经过 Dropout 处理后输出 4096 个数据。

**8、第八层（全连接层）**  
![](https://static.oschina.net/uploads/space/2018/0312/011238_2uC4_876354.png)
   
第八层的处理流程为：全连接  
![](https://static.oschina.net/uploads/space/2018/0312/011245_4BwQ_876354.png)
   
第七层输出的 4096 个数据与第八层的 1000 个神经元进行全连接，经过训练后输出 1000 个 float 型的值，这就是预测结果。

  
以上就是关于 AlexNet 网络结构图的逐层解析了，看起来挺复杂的，下面是一个简图，看起来就清爽很多啊  
![](https://static.oschina.net/uploads/space/2018/0312/011252_WjZo_876354.png)

  
通过前面的介绍，可以看出 AlexNet 的特点和创新之处，主要如下：  
![](https://static.oschina.net/uploads/space/2018/0312/011312_5vuH_876354.png)

**墙裂建议**

Alex 等人在 2012 年发表了关于 AlexNet 的经典论文《ImageNet Classification with Deep Convolutional Neural Networks》（基于深度卷积神经网络的 ImageNet 分类），整个过程都有详细的介绍，建议阅读这篇论文，进一步了解细节。

扫描以下二维码关注本人公众号 “大数据与人工智能 Lab”（BigdataAILab），然后回复 “**论文**” 关键字可在线阅读这篇经典论文的内容。

![](https://static.oschina.net/uploads/space/2018/0213/155533_IdYn_876354.jpg)

**推荐相关阅读**

*   [大话卷积神经网络（CNN）](https://my.oschina.net/u/876354/blog/1620906)
*   [大话循环神经网络（RNN）](https://my.oschina.net/u/876354/blog/1621839)
*   [大话深度残差网络（DRN）](https://my.oschina.net/u/876354/blog/1622896)
*   [大话深度信念网络（DBN）](https://my.oschina.net/u/876354/blog/1626639)
*   [大话 CNN 经典模型：LeNet](https://my.oschina.net/u/876354/blog/1632862)
*   [浅说 “迁移学习”](https://my.oschina.net/u/876354/blog/1614883)
*   [什么是 “强化学习”](https://my.oschina.net/u/876354/blog/1614879)
*   [AlphaGo 算法原理浅析](https://my.oschina.net/u/876354/blog/1594849)
*   [大数据究竟有多少个 V](https://my.oschina.net/u/876354/blog/1604254)
*   [Apache Hadoop 2.8 完全分布式集群搭建超详细教程](https://my.oschina.net/u/876354/blog/993836)
*   [Apache Hive 2.1.1 安装配置超详细教程](https://my.oschina.net/u/876354/blog/1057639)
*   [Apache HBase 1.2.6 完全分布式集群搭建超详细教程](https://my.oschina.net/u/876354/blog/1163018)
*   [离线安装 Cloudera Manager 5 和 CDH5（最新版 5.13.0）超详细教程](https://my.oschina.net/u/876354/blog/1605320)