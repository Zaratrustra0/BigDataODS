# 六千字总结:大数据框架(分区，分桶，分片),建议收藏
## 前言

在大数据分布式中，分区，分桶，分片是设计框架的重点。此篇就来总结各个框架。建议收藏

### 目录

-   Hive 分区与分桶
-   ES 分片
-   Kafka 分区
-   HBase 分区
-   Kudu 分区

## Hive

### Hive 分区

是按照数据表的某列或者某些列分为多区，在 hive 存储上是 hdfs 文件，也就是文件夹形式。现在最常用的跑 T+1 数据，按当天时间分区的较多。

把每天通过 sqoop 或者 datax 拉取的一天的数据存储一个区，也就是所谓的文件夹与文件。在查询时只要指定分区字段的值就可以直接从该分区查找即可。创建分区表的时候，要通过关键字 partitioned by （column name  string）声明该表是分区表，并且是按照字段 column name 进行分区，column name 值一致的所有记录存放在一个分区中，分区属性 name 的类型是 string 类型。

当然，可以依据多个列进行分区，即对某个分区的数据按照某些列继续分区。

向分区表导入数据的时候，要通过关键字 partition（（column name="xxxx"）显示声明数据要导入到表的哪个分区

###### 设置分区的影响

1.  首先是 hive 本身对分区数有限制，不过可以修改限制的数量；

`set hive.exec.dynamic.partition=true;  
set hive.exec.max.dynamic.partitions=1000;   
set hive.exec.dynamic.partition.mode=nonstrict;   
set hive.exec.parallel.thread.number=264;  
`

2.  hdfs 对单个目录下的目录数量或者文件数量也是有限制的，也是可以修改的；
3.  NN 的内存肯定会限制，这是最重要的，如果分区数很大，会影响 NN 服务，进而影响一系列依赖于 NN 的服务。所以最好合理设置分区规则，对小文件也可以定期合并，减少 NN 的压力。

### Hive 分桶

在分区数量过于庞大以至于可能导致文件系统崩溃时，我们就需要使用分桶来解决问题

分桶是相对分区进行更细粒度的划分。分桶则是指定分桶表的某一列，让该列数据按照哈希取模的方式随机、均匀的分发到各个桶文件中。因为分桶操作需要根据某一列具体数据来进行哈希取模操作，故指定的分桶列必须基于表中的某一列（字段） 要使用关键字 clustered by 指定分区依据的列名，还要指定分为多少桶

create table test(id int,name string) cluster by (id) into 5 buckets .......

insert into buck select id ,name from p cluster by (id)

### Hive 分区分桶区别

-   分区是表的部分列的集合，可以为频繁使用的数据建立分区，这样查找分区中的数据时就不需要扫描全表，这对于提高查找效率很有帮助
-   不同于分区对列直接进行拆分，桶往往使用列的哈希值对数据打散，并分发到各个不同的桶中从而完成数据的分桶过程
-   分区和分桶最大的区别就是分桶随机分割数据库，分区是非随机分割数据库

## ElasticSearch 分片

主分片：用于解决数据水平扩展的问题，一个索引的所有数据是分布在所有主分片之上的（每个主分片承担一部分数据，主分片又分布在不同的节点上），一个索引的主分片数量只能在创建时指定，后期无法修改，除非对数据进行重新构建索引（reindex 操作）。

副本分片：用于解决数据高可用的问题，一个副本分片即一个主分片的拷贝，其数量可以动态调整，通过增加副本分片也可以实现提升系统读性能的作用。

在集群中唯一一个空节点上创建一个叫做 blogs 的索引。默认情况下，一个索引被分配 5 个主分片

`{  
    "settings": {  
        "number_of_shards": 5,  
        "number_of_replicas": 1  
    }  
}`

到底分配到那个 shard 上呢?

`shard = hash(routing) % number_of_primary_shards  
`

routing 是一个可变值，默认是文档的 \_id ，也可以设置成一个自定义的值。routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到余数 。这个在 0 到 number_of_primary_shards 之间的余数，就是所寻求的文档所在分片的位置。

如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了

-   分片过少

    如 15 个节点，5 个主分片，1 个副本 会造成每个索引最多只能使用 10 个节点（5 个主分片，5 个从分片），剩余 5 节点并没有利用上；资源浪费

    如：3 节点；3 分主分片，1 副本 当数据量较大的时，每个分片就会比较大
-   分片过多

1.  创建分片慢：es 创建分片的速度会随着集群内分片数的增加而变慢。
2.  集群易崩溃：在触发 es 自动创建 Index 时，由于创建速度太慢，容易导致大量写入请求堆积在内存，从而压垮集群。
3.  写入拒绝：分片过多的场景中，如果不能及时掌控业务的变化，可能经常遇到单分片记录超限、写入拒绝等问题。

###### 分片的注意事项

1.  避免使用非常大的分片，因为这会对群集从故障中恢复的能力产生负面影响。对分片的大小没有固定的限制，但是通常情况下很多场景限制在 30GB 的分片大小以内。
2.  当在 ElasticSearch 集群中配置好你的索引后, 你要明白在集群运行中你无法调整分片设置. 既便以后你发现需要调整分片数量, 你也只能新建创建并对数据进行重新索引.
3.  如果担心数据的快速增长, 建议根据这条限制: ElasticSearch 推荐的最大 JVM 堆空间 是 30~32G, 所以把分片最大容量限制为 30GB, 然后再对分片数量做合理估算。例如, 如果的数据能达到 200GB, 则最多分配 7 到 8 个分片。

## kafka 分区

### 生产者

分区的原因

1.  方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了；
2.  可以提高并发，因为可以以 Partition 为单位读写了。

分区的原则

1.  指明 partition 的情况下，直接将指明的值直接作为 partiton 值；
2.  没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；
3.  既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。

### 消费者

#### 分区分配策略

一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费 Kafka 有三种分配策略，一是 RoundRobin，一是 Range。高版本还有一个 StickyAssignor 策略 将分区的所有权从一个消费者移到另一个消费者称为重新平衡（rebalance）。当以下事件发生时，Kafka 将会进行一次分区分配：

-   同一个 Consumer Group 内新增消费者
-   消费者离开当前所属的 Consumer Group，包括 shuts down 或 crashes

###### Range 分区分配策略

Range 是对每个 Topic 而言的（即一个 Topic 一个 Topic 分），首先对同一个 Topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后用 Partitions 分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。假设 n = 分区数 / 消费者数量，m = 分区数 % 消费者数量，那么前 m 个消费者每个分配 n+1 个分区，后面的（消费者数量 - m）个消费者每个分配 n 个分区。假如有 10 个分区，3 个消费者线程，把分区按照序号排列

0，1，2，3，4，5，6，7，8，9

消费者线程为

C1-0，C2-0，C2-1

那么用 partition 数除以消费者线程的总数来决定每个消费者线程消费几个 partition，如果除不尽，前面几个消费者将会多消费一个分区。在我们的例子里面，我们有 10 个分区，3 个消费者线程，10/3 = 3，而且除除不尽，那么消费者线程 C1-0 将会多消费一个分区，所以最后分区分配的结果看起来是这样的：

C1-0：0，1，2，3

C2-0：4，5，6

C2-1：7，8，9

如果有 11 个分区将会是：

C1-0：0，1，2，3

C2-0：4，5，6，7

C2-1：8，9，10

假如我们有两个主题 T1,T2，分别有 10 个分区，最后的分配结果将会是这样：

C1-0：T1（0，1，2，3） T2（0，1，2，3）

C2-0：T1（4，5，6） T2（4，5，6）

C2-1：T1（7，8，9） T2（7，8，9）

###### RoundRobinAssignor 分区分配策略

RoundRobinAssignor 策略的原理是将消费组内所有消费者以及消费者所订阅的所有 topic 的 partition 按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者. 使用 RoundRobin 策略有两个前提条件必须满足：

-   同一个消费者组里面的所有消费者的 num.streams（消费者消费线程数）必须相等；
-   每个消费者订阅的主题必须相同。

加入按照 hashCode 排序完的 topic-partitions 组依次为

T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9

我们的消费者线程排序为

C1-0, C1-1, C2-0, C2-1

最后分区分配的结果为：

C1-0 将消费 T1-5, T1-2, T1-6 分区

C1-1 将消费 T1-3, T1-1, T1-9 分区

C2-0 将消费 T1-0, T1-4 分区

C2-1 将消费 T1-8, T1-7 分区

###### StickyAssignor 分区分配策略

Kafka 从 0.11.x 版本开始引入这种分配策略，它主要有两个目的：

-   分区的分配要尽可能的均匀，分配给消费者者的主题分区数最多相差一个
-   分区的分配尽可能的与上次分配的保持相同。

当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目的，StickyAssignor 策略的具体实现要比 RangeAssignor 和 RoundRobinAssignor 这两种分配策略要复杂很多。

假设消费组内有 3 个消费者

C0、C1、C2

它们都订阅了 4 个主题：

t0、t1、t2、t3

并且每个主题有 2 个分区，也就是说整个消费组订阅了

t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1 这 8 个分区

最终的分配结果如下：

消费者 C0：t0p0、t1p1、t3p0

消费者 C1：t0p1、t2p0、t3p1

消费者 C2：t1p0、t2p1

这样初看上去似乎与采用 RoundRobinAssignor 策略所分配的结果相同

此时假设消费者 C1 脱离了消费组，那么消费组就会执行再平衡操作，进而消费分区会重新分配。如果采用 RoundRobinAssignor 策略，那么此时的分配结果如下：

消费者 C0：t0p0、t1p0、t2p0、t3p0

消费者 C2：t0p1、t1p1、t2p1、t3p1

如分配结果所示，RoundRobinAssignor 策略会按照消费者 C0 和 C2 进行重新轮询分配。而如果此时使用的是 StickyAssignor 策略，那么分配结果为：

消费者 C0：t0p0、t1p1、t3p0、t2p0

消费者 C2：t1p0、t2p1、t0p1、t3p1

可以看到分配结果中保留了上一次分配中对于消费者 C0 和 C2 的所有分配结果，并将原来消费者 C1 的 “负担” 分配给了剩余的两个消费者 C0 和 C2，最终 C0 和 C2 的分配还保持了均衡。

如果发生分区重分配，那么对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍，这显然很浪费系统资源。StickyAssignor 策略如同其名称中的 “sticky” 一样，让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗以及其它异常情况的发生。

到目前为止所分析的都是消费者的订阅信息都是相同的情况，我们来看一下订阅信息不同的情况下的处理。

举例，同样消费组内有 3 个消费者：

C0、C1、C2

集群中有 3 个主题：

t0、t1、t2

这 3 个主题分别有

1、2、3 个分区

也就是说集群中有

t0p0、t1p0、t1p1、t2p0、t2p1、t2p2 这 6 个分区

消费者 C0 订阅了主题 t0

消费者 C1 订阅了主题 t0 和 t1

消费者 C2 订阅了主题 t0、t1 和 t2

如果此时采用 RoundRobinAssignor 策略：

消费者 C0：t0p0

消费者 C1：t1p0

消费者 C2：t1p1、t2p0、t2p1、t2p2

如果此时采用的是 StickyAssignor 策略：

消费者 C0：t0p0

消费者 C1：t1p0、t1p1

消费者 C2：t2p0、t2p1、t2p2

此时消费者 C0 脱离了消费组，那么 RoundRobinAssignor 策略的分配结果为：

消费者 C1：t0p0、t1p1

消费者 C2：t1p0、t2p0、t2p1、t2p2

StickyAssignor 策略，那么分配结果为：

消费者 C1：t1p0、t1p1、t0p0

消费者 C2：t2p0、t2p1、t2p2

可以看到 StickyAssignor 策略保留了消费者 C1 和 C2 中原有的 5 个分区的分配：

t1p0、t1p1、t2p0、t2p1、t2p2。

从结果上看 StickyAssignor 策略比另外两者分配策略而言显得更加的优异，这个策略的代码实现也是异常复杂。

##### 注意

在实际开发过程中，kafka 与 spark 或者 flink 对接的较多，一个分区对应的是一个并行度，如果并行度不够，这个时候会多个分区数据集中到一个并行度上。所以需要合理设置并行度

## HBase 分区

HBase 每张表在底层存储上是由至少一个 Region 组成，Region 实际上就是 HBase 表的分区。HBase 新建一张表时默认 Region 即分区的数量为 1，一般在生产环境中我们都会手动给 Table 提前做 “预分区”，使用合适的分区策略创建好一定数量的分区并使分区均匀分布在不同 regionserver 上。一个分区在达到一定大小时会自动 Split，一分为二

###### HBase 分区过多有哪些影响：

-   频繁刷写：我们知道 Region 的一个列族对应一个 MemStore，假设 HBase 表都有统一的 1 个列族配置，则每个 Region 只包含一个 MemStore。通常 HBase 的一个 MemStore 默认大小为 128 MB，见参数 hbase.hregion.memstore.flush.size。当可用内存足够时，每个 MemStore 可以分配 128 MB 空间。当可用内存紧张时，假设每个 Region 写入压力相同，则理论上每个 MemStore 会平均分配可用内存空间。因此，当节点 Region 过多时，每个 MemStore 分到的内存空间就会很小。这个时候，写入很小的数据量就会被强制 Flush 到磁盘，将会导致频繁刷写。频繁刷写磁盘，会对集群 HBase 与 HDFS 造成很大的压力，可能会导致不可预期的严重后果。
-   压缩风暴：因 Region 过多导致的频繁刷写，将在磁盘上产生非常多的 HFile 小文件，当小文件过多的时候 HBase 为了优化查询性能就会做 Compaction 操作，合并 HFile 减少文件数量。当小文件一直很多的时候，就会出现 “压缩风暴”。Compaction 非常消耗系统 io 资源，还会降低数据写入的速度，严重的会影响正常业务的进行。
-   MSLAB 内存消耗较大：MSLAB（MemStore-local allocation buffer）存在于每个 MemStore 中，主要是为了解决 HBase 内存碎片问题，默认会分配 2 MB 的空间用于缓存最新数据。如果 Region 数量过多，MSLAB 总的空间占用就会比较大。比如当前节点有 1000 个包含 1 个列族的 Region，MSLAB 就会使用 1.95GB 的堆内存，即使没有数据写入也会消耗这么多内存。
-   Master assign region 时间较长：HBase Region 过多时 Master 分配 Region 的时间将会很长。特别体现在重启 HBase 时 Region 上线时间较长，严重的会达到小时级，造成业务长时间等待的后果。
-   影响 MapReduce 并发数：当使用 MapReduce 操作 HBase 时，通常 Region 数量就是 MapReduce 的任务数，Region 数量过多会导致并发数过多，产生过多的任务。任务太多将会占用大量资源，当操作包含很多 Region 的大表时，占用过多资源会影响其他任务的执行。

###### 具体计算 HBase 合理分区数量

`((RS memory) * (total memstore fraction)) / ((memstore size)*(column families))  
`

| 字段                      | 解释                                                                                                                                                              |
| ----------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| RS memory               | 表示 regionserver 堆内存大小，即 HBASE_HEAPSIZE                                                                                                                          |
| total memstore fraction | 表示所有 MemStore 占 HBASE_HEAPSIZE 的比例，HBase0.98 版本以后由 hbase.regionserver.global.memstore.size 参数控制，老版本由 hbase.regionserver.global.memstore.upperLimit 参数控制，默认值 0.4 |
| memstore size           | 即每个 MemStore 的大小，原生 HBase 中默认 128M                                                                                                                              |
| column families         | 即表的列族数量，通常情况下只设置 1 个，最多不超过 3 个                                                                                                                                  |

假如一个集群中每个 regionserver 的堆内存是 32GB，那么节点上最理想的 Region 数量应该是 32768\*0.4/128 ≈ 102，所以，当前环境中单节点理想情况下大概有 102 个 Region 最理想情况是假设每个 Region 上的填充率都一样，包括数据写入的频次、写入数据的大小，但实际上每个 Region 的负载各不相同，可能有的 Region 特别活跃负载特别高，有的 Region 则比较空闲。所以，通常我们认为 2-3 倍的理想 Region 数量也是比较合理的，针对上面举例来说，大概 200-300 个 Region 算是合理的。

如果实际的 Region 数量比 2~3 倍的计算值还要多，就要实际观察 Region 的刷写、压缩情况了，Region 越多则风险越大。经验告诉我们，如果单节点 Region 数量过千，集群可能存在较大风险

## Kudu 分区

为了提供可扩展性，Kudu 表被划分为称为 tablets 的单元，并分布在许多 tablet servers 上。行总是属于单个 tablet 。将行分配给 tablet 的方法由在表创建期间设置的表的分区决定。kudu 提供了 3 种分区方式：

-   Range Partitioning(范围分区) 范围分区可以根据存入数据的数据量，均衡的存储到各个机器上，防止机器出现负载不均衡现象

`create table people(id Type.INT32, name Type.STRING , age Type.INT32)  
RANGE (age) (  
    PARTITION 0 <= VALUES < 10,  
    PARTITION 10 <= VALUES < 20,  
    PARTITION 20 <= VALUES < 30,  
    PARTITION 30 <= VALUES < 40,  
    PARTITION 40 <= VALUES < 50,  
    PARTITION 50 <= VALUES < 60,  
    PARTITION 60 <= VALUES < 70,  
    PARTITION 70 <= VALUES < 80,  
    PARTITION 80 <= VALUES < 120  
)  
`

-   Hash Partitioning(哈希分区) 哈希分区通过哈希值将行分配到许多 buckets ( 存储桶 ) 之一；哈希分区是一种有效的策略，当不需要对表进行有序访问时。哈希分区对于在 tablet 之间随机散布这些功能是有效的，这有助于减轻热点和 tablet 大小不均匀。

`create table rangeTable(id Type.INT32, name Type.STRING , age Type.INT32)  
HASH (id) PARTITIONS 5,  
RANGE (id) (  
    PARTITION UNBOUNDED  
)  
`

-   Multilevel Partitioning(多级分区)

`create table rangeTable(id Type.INT32, name Type.STRING , age Type.INT32)  
HASH (age) PARTITIONS 5,  
RANGE (age) (  
    PARTITION 0 <= VALUES < 10,  
    PARTITION 10 <= VALUES < 20,  
    PARTITION 20 <= VALUES < 30,  
    PARTITION 30 <= VALUES < 40,  
    PARTITION 40 <= VALUES < 50,  
    PARTITION 50 <= VALUES < 60,  
    PARTITION 60 <= VALUES < 70,  
    PARTITION 70 <= VALUES < 80,  
    PARTITION 80 <= VALUES < 120  
`

哈希分区有利于最大限度地提高写入吞吐量，而范围分区可避免 tablet 无限增长的问题；hash 分区和 range 分区结合，可以极大提升 kudu 性能

## 总结

优秀的设计思想需要深入的研究与发现。工作实践总结是知识积累的很好途径。

关注回复关键字 “大数据” 获取海量学习资料，持续不断收集充实

![](https://mmbiz.qpic.cn/mmbiz_png/2gY1hzDz7dQhibChlIblsvU1NFBQmsInWPIAPiab52Jed7ElXDrAjGWprdmgsIQceBRTnH8yQuG5e0tDRoVW4ZoA/640?wx_fmt=png)

同学共进，点赞，转发，在看，关注，是我学习之动力。和我联系吧 ---> 交流大数据知识, 一起成长\~\~~ 
 [https://mp.weixin.qq.com/s/8n0LwZQHc7xPsyd-dBDD2Q](https://mp.weixin.qq.com/s/8n0LwZQHc7xPsyd-dBDD2Q)
