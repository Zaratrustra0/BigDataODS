# 图解机器学习 | 朴素贝叶斯算法详解
引言
--

在众多机器学习分类算法中，本篇我们提到的朴素贝叶斯模型，和其他绝大多数分类算法都不同，也是很重要的模型之一。

在机器学习中如KNN、逻辑回归、决策树等模型都是判别方法，也就是直接学习出特征输出 ![](https://www.zhihu.com/equation?tex=Y)和特征 ![](https://www.zhihu.com/equation?tex=X)之间的关系（决策函数 ![](https://www.zhihu.com/equation?tex=Y%3D%20f%28X%29)或者条件分布 ![](https://www.zhihu.com/equation?tex=P%28Y%7CX%29)）。但朴素贝叶斯是生成方法，它直接找出特征输出 ![](https://www.zhihu.com/equation?tex=Y)和特征 ![](https://www.zhihu.com/equation?tex=X)的联合分布 ![](https://www.zhihu.com/equation?tex=P%28X%2CY%29)，进而通过 ![](https://www.zhihu.com/equation?tex=P%28Y%20%5Cmid%20X%29%3D%20%5Cfrac%7BP%28X%2CY%29%7D%7BP%28X%29%7D)计算得出结果判定。

朴素贝叶斯是一个非常直观的模型，在很多领域有广泛的应用，比如早期的文本分类，很多时候会用它作为 baseline 模型，本篇内容我们对朴素贝叶斯算法原理做展开介绍。

1. 朴素贝叶斯算法核心思想
-------------

**贝叶斯分类是一类分类算法的总称**，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而**朴素贝叶斯（Naive Bayes）分类是贝叶斯分类中最简单，也是常见的一种分类方法**。

朴素贝叶斯算法的核心思想是通过考虑特征概率来预测分类，即对于给出的待分类样本，求解在此样本出现的条件下各个类别出现的概率，哪个最大，就认为此待分类样本属于哪个类别。

![](https://img-blog.csdnimg.cn/img_convert/293af29a06d01c9fdbca2321ea012896.png)

**举个例子：眼前有100个西瓜，好瓜和坏瓜个数差不多，现在要用这些西瓜来训练一个「坏瓜识别器」，我们要怎么办呢**？

一般挑西瓜时通常要「敲一敲」，听听声音，是清脆声、浊响声、还是沉闷声。所以，我们先简单点考虑这个问题，只用敲击的声音来辨别西瓜的好坏。根据经验，敲击声「清脆」说明西瓜还不够熟，敲击声「沉闷」说明西瓜成熟度好，更甜更好吃。

![](https://img-blog.csdnimg.cn/img_convert/4effdf0a1072fb146391094fbf553c45.png)

所以，坏西瓜的敲击声是「清脆」的概率更大，好西瓜的敲击声是「沉闷」的概率更大。当然这并不绝对——我们千挑万选地「沉闷」瓜也可能并没熟，这就是噪声了。当然，在实际生活中，除了敲击声，我们还有其他可能特征来帮助判断，例如色泽、跟蒂、品类等。

**朴素贝叶斯把类似「敲击声」这样的特征概率化，构成一个「西瓜的品质向量」以及对应的「好瓜/坏瓜标签」，训练出一个标准的「基于统计概率的好坏瓜模型」，这些模型都是各个特征概率构成的**。

![](https://img-blog.csdnimg.cn/img_convert/4a4c5ec3c9214e977cd55e01158f97ce.png)

这样，在面对未知品质的西瓜时，我们迅速获取了特征，分别输入「好瓜模型」和「坏瓜模型」，得到两个概率值。如果「坏瓜模型」输出的概率值大一些，那这个瓜很有可能就是个坏瓜。

2. 贝叶斯公式与条件独立假设
--------------

贝叶斯定理中很重要的概念是**先验概率**、**后验概率**和**条件概率**。（关于这部分依赖的数学知识，大家可以查看ShowMeAI的文章 [图解AI数学基础 | **概率与统计**](https://www.showmeai.tech/article-detail/163)，也可以下载我们的速查手册 [AI知识技能速查 | **数学基础-概率统计知识**](https://www.showmeai.tech/article-detail/118)）

### 1）先验概率与后验概率

**先验概率**：**事件发生前的预判概率**。可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。一般都是单独事件概率。

> 举个例子：如果我们对西瓜的色泽、根蒂和纹理等特征一无所知，按照常理来说，西瓜是好瓜的概率是 ![](https://www.zhihu.com/equation?tex=60%20%5C%25)。那么这个概率 *P(好瓜)* 就被称为先验概率。

**后验概率**：**事件发生后求的反向条件概率**。或者说，基于先验概率求得的反向条件概率。概率形式与条件概率相同。

> 举个例子：假如我们了解到判断西瓜是否好瓜的一个指标是纹理。一般来说，纹理清晰的西瓜是好瓜的概率大一些，大概是 ![](https://www.zhihu.com/equation?tex=75%20%5C%25)。如果把纹理清晰当作一种结果，然后去推测好瓜的概率，那么这个概率 *P(好瓜｜纹理清晰)* 就被称为后验概率。

**条件概率**：一个事件发生后另一个事件发生的概率。一般的形式为 ![](https://www.zhihu.com/equation?tex=P%28B%7CA%29)表示 ![](https://www.zhihu.com/equation?tex=A)发生的条件下 ![](https://www.zhihu.com/equation?tex=B)发生的概率。

![](https://img-blog.csdnimg.cn/img_convert/d20e684929c0f96bef2375f6c3114f2f.png)

### 2）贝叶斯公式

简单来说，贝叶斯定理（Bayes Theorem，也称贝叶斯公式）是基于假设的先验概率、给定假设下观察到不同数据的概率，提供了一种计算后验概率的方法。在人工智能领域，有一些概率型模型会依托于贝叶斯定理，比如我们今天的主角「朴素贝叶斯模型」。

![](https://img-blog.csdnimg.cn/img_convert/6808c17ad2240387994fca4b08f35e23.png)

*   ![](https://www.zhihu.com/equation?tex=P%28A%29)
     是先验概率，一般都是人主观给出的。贝叶斯中的先验概率一般特指它。
    
*   ![](https://www.zhihu.com/equation?tex=P%28B%29)
     是先验概率，在贝叶斯的很多应用中不重要（因为只要最大后验不求绝对值），需要时往往用全概率公式计算得到。
    
*   ![](https://www.zhihu.com/equation?tex=P%28B%20%5Cmid%20A%29)
     是条件概率，又叫似然概率，一般是通过历史数据统计得到。
    
*   ![](https://www.zhihu.com/equation?tex=P%28A%20%5Cmid%20B%29)
     是后验概率，一般是我们求解的目标。
    

### 3）条件独立假设与朴素贝叶斯

基于贝叶斯定理的贝叶斯模型是一类简单常用的分类算法。在「假设待分类项的各个属性相互独立」的情况下，构造出来的分类算法就称为朴素的，即朴素贝叶斯算法。

**所谓「朴素」，是假定所有输入事件之间是相互独立**。进行这个假设是因为独立事件间的概率计算更简单。

**朴素贝叶斯模型的基本思想**：对于给定的待分类项 ![](https://www.zhihu.com/equation?tex=X%20%5Cleft%5C%7B%20%20a_1%2Ca_2%2Ca_3%2C%E2%8B%AF%2Ca_n%20%5Cright%5C%7D)，求解在此项出现的条件下各个类别 ![](https://www.zhihu.com/equation?tex=y_i)出现的概率，哪个 ![](https://www.zhihu.com/equation?tex=P%28y_i%20%7CX%29)最大，就把此待分类项归属于哪个类别。

**朴素贝叶斯算法的定义**：设 ![](https://www.zhihu.com/equation?tex=X%20%5Cleft%5C%7B%20%20a_1%2Ca_2%2Ca_3%2C%E2%8B%AF%2Ca_n%20%5Cright%5C%7D)为一个待分类项，每个 ![](https://www.zhihu.com/equation?tex=a_%7Bi%7D)为 ![](https://www.zhihu.com/equation?tex=x)的一个特征属性，且特征属性之间相互独立。设 ![](https://www.zhihu.com/equation?tex=C%20%5Cleft%5C%7B%20y_1%2Cy_2%2Cy_3%2C%E2%8B%AF%2Cy_n%5Cright%5C%7D)为一个类别集合，计算

![](https://www.zhihu.com/equation?tex=P%5Cleft%28y_%7B1%7D%20%5Cmid%20X%5Cright%29%2C%20P%5Cleft%28y_%7B2%7D%20%5Cmid%20X%5Cright%29%2C%20P%5Cleft%28y_%7B3%7D%20%5Cmid%20X%5Cright%29%2C%20%5Cldots%2C%20P%5Cleft%28y_%7Bn%7D%20%5Cmid%20X%5Cright%29)

![](https://www.zhihu.com/equation?tex=P%5Cleft%28y_%7Bk%7D%20%5Cmid%20X%5Cright%29%3D%5Cmax%20%5Cleft%5C%7BP%5Cleft%28y_%7B1%7D%20%5Cmid%20X%5Cright%29%2C%20P%5Cleft%28y_%7B2%7D%20%5Cmid%20X%5Cright%29%2C%20P%5Cleft%28y_%7B3%7D%20%5Cmid%20X%5Cright%29%2C%20%5Cldots%2C%20P%5Cleft%28y_%7Bn%7D%20%5Cmid%20X%5Cright%29%5Cright%5C%7D%20%2C%20%E5%88%99X%20%5Cin%20y_%7Bk%7D)

![](https://img-blog.csdnimg.cn/img_convert/f191f10a906db90693e8bc60a1d79ef2.png)

要求出第四项中的后验概率 ![](https://www.zhihu.com/equation?tex=P%5Cleft%28y_%7Bk%7D%20%5Cmid%20X%5Cright%29)
，就需要分别求出在第三项中的各个条件概率，其步骤是：

*   找到一个已知分类的待分类项集合，这个集合叫做训练样本集
*   统计得到在各类别下各个特征属性的条件概率估计。即

![](https://www.zhihu.com/equation?tex=P%5Cleft%28a_%7B1%7D%20%5Cmid%20y_%7B1%7D%5Cright%29%2C%20P%5Cleft%28a_%7B2%7D%20%5Cmid%20y_%7B1%7D%5Cright%29%2C%20%5Ccdots%2C%20P%5Cleft%28a_%7Bn%7D%20%5Cmid%20y_%7B1%7D%5Cright%29)

![](https://www.zhihu.com/equation?tex=P%5Cleft%28a_%7B1%7D%20%5Cmid%20y_%7B2%7D%5Cright%29%2C%20P%5Cleft%28a_%7B2%7D%20%5Cmid%20y_%7B2%7D%5Cright%29%2C%20%5Ccdots%2C%20P%5Cleft%28a_%7Bn%7D%20%5Cmid%20y_%7B2%7D%5Cright%29)

![](https://www.zhihu.com/equation?tex=%5Cldots)

![](https://www.zhihu.com/equation?tex=P%5Cleft%28a_%7B1%7D%20%5Cmid%20y_%7Bn%7D%5Cright%29%2C%20P%5Cleft%28a_%7B2%7D%20%5Cmid%20y_%7Bn%7D%5Cright%29%2C%20%5Ccdots%2C%20P%5Cleft%28a_%7Bn%7D%20%5Cmid%20y_%7Bn%7D%5Cright%29)

![](https://img-blog.csdnimg.cn/img_convert/bba287b5efd88833fc5920f338db0b2a.png)

在朴素贝叶斯算法中，待分类项的每个特征属性都是条件独立的，由贝叶斯公式

![](https://www.zhihu.com/equation?tex=P%5Cleft%28y_%7Bi%7D%20%5Cmid%20X%5Cright%29%3D%5Cfrac%7BP%5Cleft%28X%20%5Cmid%20y_%7Bi%7D%5Cright%29%20P%5Cleft%28y_%7Bi%7D%5Cright%29%7D%7BP%28X%29%7D)

因为分母相当于在数据库中 ![](https://www.zhihu.com/equation?tex=X)存在的概率，所以对于任何一个待分类项来说 ![](https://www.zhihu.com/equation?tex=P%5Cleft%28X%20%5Cright%29)都是常数固定的。再求后验概率 ![](https://www.zhihu.com/equation?tex=P%5Cleft%28y_%7Bi%7D%20%5Cmid%20X%5Cright%29)的时候只用考虑分子即可。

因为各特征值是独立的所以有：

![](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D%20P%5Cleft%28X%20%5Cmid%20y_%7Bi%7D%5Cright%29%20P%5Cleft%28y_%7Bi%7D%5Cright%29%20%26%3DP%5Cleft%28a_%7B1%7D%20%5Cmid%20y_%7Bi%7D%5Cright%29%20P%5Cleft%28a_%7B2%7D%20%5Cmid%20y_%7Bi%7D%5Cright%29%20%5Ccdots%20P%5Cleft%28a_%7Bn%7D%20%5Cmid%20y_%7Bi%7D%5Cright%29%20P%5Cleft%28y_%7Bi%7D%5Cright%29%20%5C%5C%20%26%3DP%5Cleft%28y_%7Bi%7D%5Cright%29%20%5Cprod_%7Bj%3D1%7D%5E%7Bn%7D%20P%5Cleft%28a_%7Bj%7D%20%5Cmid%20y_%7Bi%7D%5Cright%29%20%5Cend%7Baligned%7D)

可以推出：

![](https://www.zhihu.com/equation?tex=P%5Cleft(X%20%5Cmid%20y_%7Bi%7D%5Cright)%3D%5Cprod_%7B%7Bk%3D1%7D%7D%5E%7Bn%7D%20P%5Cleft(a_%7Bk%7D%20%5Cmid%20y_%7Bi%7D%5Cright))

![](https://img-blog.csdnimg.cn/img_convert/75235ad502f0129bb8ce1f59850aa91a.png)

对于 ![](https://www.zhihu.com/equation?tex=P%5Cleft%28y_%7Bi%7D%5Cright%29)是指在训练样本中 ![](https://www.zhihu.com/equation?tex=y_%7Bi%7D)出现的概率，可以近似的求解为：

![](https://www.zhihu.com/equation?tex=P%5Cleft%28y_%7Bi%7D%5Cright%29%3D%5Cfrac%7B%5Cleft%7Cy_%7Bi%7D%5Cright%7C%7D%7BD%7D)

对于先验概率 ![](https://www.zhihu.com/equation?tex=P%5Cleft%20%28%20a_%7Bj%7D%20%5Cmid%20y_%7Bi%7D%20%5Cright%20%29)，是指在类别 ![](https://www.zhihu.com/equation?tex=y_%7Bi%7D)中，特征元素 ![](https://www.zhihu.com/equation?tex=a_%7Bj%7D)出现的概率，可以求解为：

![](https://www.zhihu.com/equation?tex=P%5Cleft%20%28%20a_%7Bj%7D%20%5Cmid%20y_%7Bi%7D%20%5Cright%20%29%20%3D%20%5Cfrac%7B%5Cleft%20%7C%20%E5%9C%A8%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E4%B8%BA%20y_%7Bi%7D%20%E6%97%B6%EF%BC%8Ca_%7Bj%7D%20%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%20%5Cright%20%7C%20%7D%7B%5Cleft%20%7C%20y_%7Bi%7D%20%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E6%95%B0%20%5Cright%20%7C%20%7D)

![](https://img-blog.csdnimg.cn/img_convert/f5f98b009e5ab31f6262b2812d6d19bf.png)

总结一下，朴素贝叶斯模型的分类过程如下流程图所示：

![](https://img-blog.csdnimg.cn/img_convert/d8a68e35abdf816f69c2d1e2647a7274.png)

3. 伯努利与多项式朴素贝叶斯
--------------

### 1）多项式vs伯努利朴素贝叶斯

大家在一些资料中，会看到「多项式朴素贝叶斯」和「伯努利朴素贝叶斯」这样的细分名称，我们在这里基于文本分类来给大家解释一下：

![](https://img-blog.csdnimg.cn/img_convert/cf754bc75251e04473808c071cffd2f9.png)

在文本分类的场景下使用朴素贝叶斯，那对应的特征 ![](https://www.zhihu.com/equation?tex=a_j)
 就是单词，对应的类别标签就是 ![](https://www.zhihu.com/equation?tex=y)
，这里有一个问题：每个单词会出现很多次，我们对于频次有哪些处理方法呢？

*   如果直接以**单词的频次**参与统计计算，那就是**多项式**朴素贝叶斯的形态。
*   如果以**是否出现(0和1)**参与统计计算，就是**伯努利**朴素贝叶斯的形态。

![](https://img-blog.csdnimg.cn/img_convert/470b40166bccfc01b86283916be76f7a.png)

#### （1）多项式朴素贝叶斯

以文本分类为例，多项式模型如下。在多项式模型中，设某文档 ![](https://www.zhihu.com/equation?tex=d%3D%5Cleft%28t_%7B1%7D%2C%20t_%7B2%7D%2C%20%5Cldots%2C%20t_%7Bk%7D%5Cright%29)
，![](https://www.zhihu.com/equation?tex=t_%7Bk%7D)
 是该文档中出现过的单词，允许重复，则：

先验概率

![](https://www.zhihu.com/equation?tex=P%5Cleft%20%28%20c%20%5Cright%20%29%20%3D%20%5Cfrac%7B%E7%B1%BBc%E4%B8%8B%E5%8D%95%E8%AF%8D%E6%80%BB%E6%95%B0%7D%7B%E6%95%B4%E4%B8%AA%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E7%9A%84%E5%8D%95%E8%AF%8D%E6%80%BB%E6%95%B0%7D)

类条件概率

![](https://www.zhihu.com/equation?tex=P%5Cleft%20%28%20t_%7Bk%7D%20%5Cmid%20c%20%5Cright%20%29%20%3D%20%5Cfrac%7B%E7%B1%BBc%E4%B8%8B%E5%8D%95%E8%AF%8Dt_%7Bk%7D%E5%9C%A8%E5%90%84%E4%B8%AA%E6%96%87%E6%A1%A3%E4%B8%AD%E5%87%BA%E7%8E%B0%E8%BF%87%E7%9A%84%E6%AC%A1%E6%95%B0%E4%B9%8B%E5%92%8C%2B1%7D%7B%E7%B1%BBc%E4%B8%8B%E5%8D%95%E8%AF%8D%E6%80%BB%E6%95%B0%2B%5Cleft%20%7C%20V%20%5Cright%20%7C%7D)

#### （2）伯努利朴素贝叶斯

对应的，在伯努利朴素贝叶斯里，我们假设各个特征在各个类别下是服从n重伯努利分布（二项分布）的，因为伯努利试验仅有两个结果，因此，算法会首先对特征值进行二值化处理（假设二值化的结果为1与0）。

对应的 ![](https://www.zhihu.com/equation?tex=P%20%5Cleft%20%28%20c%20%5Cright%20%29)和 ![](https://www.zhihu.com/equation?tex=P%5Cleft%20%28%20t_%7Bk%7D%20%5Cmid%20c%20%5Cright%20%29)计算方式如下（注意到分子分母的变化)：

![](https://www.zhihu.com/equation?tex=P%20%5Cleft%20%28%20c%20%5Cright%20%29%3D%5Cfrac%7B%E7%B1%BBc%E4%B8%8B%E6%96%87%E4%BB%B6%E6%80%BB%E6%95%B0%7D%7B%E6%95%B4%E4%B8%AA%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E7%9A%84%E6%96%87%E4%BB%B6%E6%80%BB%E6%95%B0%7D)

![](https://www.zhihu.com/equation?tex=P%5Cleft%20%28%20t_%7Bk%7D%20%5Cmid%20c%20%5Cright%20%29%20%3D%20%5Cfrac%7B%E7%B1%BBc%E4%B8%8B%E5%8D%95%E8%AF%8Dt_%7Bk%7D%E5%9C%A8%E5%90%84%E4%B8%AA%E6%96%87%E6%A1%A3%E4%B8%AD%E5%87%BA%E7%8E%B0%E8%BF%87%E7%9A%84%E6%AC%A1%E6%95%B0%E4%B9%8B%E5%92%8C%2B1%7D%7B%E7%B1%BBc%E4%B8%8B%E5%8D%95%E8%AF%8D%E6%80%BB%E6%95%B0%2B2%7D)

### 2）朴素贝叶斯与连续值特征

我们发现在之前的概率统计方式，都是基于离散值的。如果遇到连续型变量特征，怎么办呢？

以人的身高，物体的长度为例。一种处理方式是：把它转换成离散型的值。比如：

当然有不同的转换方法，比如还可以：

但是，以上的划分方式，都比较粗糙，划分的规则也是人为拟定的，且在同一区间内的样本（比如第1套变换规则下，身高 ![](https://www.zhihu.com/equation?tex=150cm)和 ![](https://www.zhihu.com/equation?tex=155cm))难以区分，我们有高斯朴素贝叶斯模型可以解决这个问题。

如果特征 ![](https://www.zhihu.com/equation?tex=x_%7Bi%7D)是连续变量，如何去估计似然度 ![](https://www.zhihu.com/equation?tex=P%5Cleft%20%28%20x_%7Bi%7D%5Cmid%20y_%7Bk%7D%20%5Cright%20%29)呢？**高斯模型**是这样做的：我们假设在 ![](https://www.zhihu.com/equation?tex=y_%7Bi%7D)的条件下，![](https://www.zhihu.com/equation?tex=x)服从高斯分布（正态分布）。根据正态分布的概率密度函数即可计算出 ![](https://www.zhihu.com/equation?tex=P%5Cleft%20%28%20x%20%5Cmid%20y_%7Bi%7D%20%5Cright%20%29)，公式如下：

![](https://www.zhihu.com/equation?tex=P%5Cleft%28x_%7Bi%7D%20%5Cmid%20y_%7Bk%7D%5Cright%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma_%7By%20k%2C%20i%7D%5E%7B2%7D%7D%7D%20e%5E%7B-%5Cfrac%7B%5Cleft%28x_%7Bi%7D-%5Cmu_%7By%20k%2C%20i%7D%5Cright%29%5E%7B2%7D%7D%7B2%20%5Csigma_%7By%20k%2C%20i%7D%5E%7B2%7D%7D%7D)

![](https://img-blog.csdnimg.cn/img_convert/2dc3deabd3d9884fe14e8fecca6e3a70.png)

回到上述例子，如果身高是我们判定人性别（男/女）的特征之一，我们可以假设男性和女性的身高服从正态分布，通过样本计算出身高均值和方差，对应上图中公式就得到正态分布的密度函数。有了密度函数，遇到新的身高值就可以直接代入，算出密度函数的值。

4. 平滑处理
------

### 1）为什么需要平滑处理

使用朴素贝叶斯，有时候会面临零概率问题。零概率问题，指的是在计算实例的概率时，如果某个量 ![](https://www.zhihu.com/equation?tex=x)
，在观察样本库（训练集)中没有出现过，会导致整个实例的概率结果是 0 。

在文本分类的问题中，当「一个词语没有在训练样本中出现」时，这个词基于公式统计计算得到的条件概率为 0 ，使用连乘计算文本出现概率时也为 0 。这是不合理的，不能因为一个事件没有观察到就武断的认为该事件的概率是 0 。

### 2）拉普拉斯平滑及依据

为了解决零概率的问题，法国数学家拉普拉斯最早提出用加1的方法估计没有出现过的现象的概率，所以加法平滑也叫做拉普拉斯平滑。

假定训练样本很大时，每个分量 ![](https://www.zhihu.com/equation?tex=x)的计数加 1 造成的估计概率变化可以忽略不计，但可以方便有效的避免零概率问题。

对应到文本分类的场景中，如果使用多项式朴素贝叶斯，假定特征 ![](https://www.zhihu.com/equation?tex=x_%7Bi%7D)表示某个词在样本中出现的次数（当然用TF-IDF表示也可以)。拉普拉斯平滑处理后的条件概率计算公式为：

![](https://www.zhihu.com/equation?tex=P%5Cleft%28x_%7Bi%7D%20%5Cmid%20y%5Cright%29%20%3D%5Cfrac%7BN_%7By%20i%7D%2B%5Calpha%7D%7BN_%7By%7D%2Bn%20%5Calpha%7D)

![](https://img-blog.csdnimg.cn/img_convert/8d91d711eab335b571c7290be8cabcd9.png)

更多监督学习的算法模型总结可以查看ShowMeAI的文章 [AI知识技能速查 | **机器学习-监督学习**](https://www.showmeai.tech/article-detail/113)。

机器学习【算法】系列教程
------------

*   [图解机器学习 | 机器学习基础知识](https://www.showmeai.tech/article-detail/185)
*   [图解机器学习 | 模型评估方法与准则](https://www.showmeai.tech/article-detail/186)
*   [图解机器学习 | KNN算法及其应用](https://www.showmeai.tech/article-detail/187)
*   [图解机器学习 | 逻辑回归算法详解](https://www.showmeai.tech/article-detail/188)
*   [图解机器学习 | 朴素贝叶斯算法详解](https://www.showmeai.tech/article-detail/189)
*   [图解机器学习 | 决策树模型详解](https://www.showmeai.tech/article-detail/190)
*   [图解机器学习 | 随机森林分类模型详解](https://www.showmeai.tech/article-detail/191)
*   [图解机器学习 | 回归树模型详解](https://www.showmeai.tech/article-detail/192)
*   [图解机器学习 | GBDT模型详解](https://www.showmeai.tech/article-detail/193)
*   [图解机器学习 | XGBoost模型最全解析](https://www.showmeai.tech/article-detail/194)
*   [图解机器学习 | LightGBM模型详解](https://www.showmeai.tech/article-detail/195)
*   [图解机器学习 | 支持向量机模型详解](https://www.showmeai.tech/article-detail/196)
*   [图解机器学习 | 聚类算法详解](https://www.showmeai.tech/article-detail/197)
*   [图解机器学习 | PCA降维算法详解](https://www.showmeai.tech/article-detail/198)

机器学习【实战】系列教程
------------

*   [机器学习实战 | Python机器学习算法应用实践](https://www.showmeai.tech/article-detail/201)
*   [机器学习实战 | SKLearn入门与简单应用案例](https://www.showmeai.tech/article-detail/202)
*   [机器学习实战 | SKLearn最全应用指南](https://www.showmeai.tech/article-detail/203)
*   [机器学习实战 | XGBoost建模应用详解](https://www.showmeai.tech/article-detail/204)
*   [机器学习实战 | LightGBM建模应用详解](https://www.showmeai.tech/article-detail/205)
*   [机器学习实战 | Python机器学习综合项目-电商销量预估](https://www.showmeai.tech/article-detail/206)
*   [机器学习实战 | Python机器学习综合项目-电商销量预估<进阶方案>](https://www.showmeai.tech/article-detail/207)
*   [机器学习实战 | 机器学习特征工程最全解读](https://www.showmeai.tech/article-detail/208)
*   [机器学习实战 | 自动化特征工程工具Featuretools应用](https://www.showmeai.tech/article-detail/209)
*   [机器学习实战 | AutoML自动化机器学习建模](https://www.showmeai.tech/article-detail/210)

[ShowMeAI](https://www.showmeai.tech/) 系列教程推荐
---------------------------------------------

*   [大厂技术实现：推荐与广告计算解决方案](https://www.showmeai.tech/tutorials/50)
*   [大厂技术实现：计算机视觉解决方案](https://www.showmeai.tech/tutorials/51)
*   [大厂技术实现：自然语言处理行业解决方案](https://www.showmeai.tech/tutorials/52)
*   [图解Python编程：从入门到精通系列教程](https://www.showmeai.tech/tutorials/56)
*   [图解数据分析：从入门到精通系列教程](https://www.showmeai.tech/tutorials/33)
*   [图解AI数学基础：从入门到精通系列教程](https://www.showmeai.tech/tutorials/83)
*   [图解大数据技术：从入门到精通系列教程](https://www.showmeai.tech/tutorials/84)
*   [图解机器学习算法：从入门到精通系列教程](https://www.showmeai.tech/tutorials/34)
*   [机器学习实战：手把手教你玩转机器学习系列](https://www.showmeai.tech/tutorials/41)
*   [深度学习教程：吴恩达专项课程 · 全套笔记解读](https://www.showmeai.tech/tutorials/35)
*   [自然语言处理教程：斯坦福CS224n课程 · 课程带学与全套笔记解读](https://www.showmeai.tech/tutorials/36)