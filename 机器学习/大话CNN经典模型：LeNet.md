# 大话CNN经典模型：LeNet
![](https://static.oschina.net/uploads/space/2018/0311/012810_naQu_876354.png)

近几年来，卷积神经网络（Convolutional Neural Networks，简称 CNN）在图像识别中取得了非常成功的应用，成为深度学习的一大亮点。CNN 发展至今，已经有很多变种，其中有几个经典模型在 CNN 发展历程中有着里程碑的意义，它们分别是：LeNet、Alexnet、Googlenet、VGG、DRL 等，接下来将分期进行逐一介绍。  
在之前的文章中，已经介绍了卷积神经网络（CNN）的技术原理，细节部分就不再重复了，有兴趣的同学再打开链接看看（[大话卷积神经网络](https://my.oschina.net/u/876354/blog/1620906)），在此简单回顾一下 CNN 的几个特点：局部感知、参数共享、池化。  
**1、局部感知**  
人类对外界的认知一般是从局部到全局、从片面到全面，类似的，在机器识别图像时也没有必要把整张图像按像素全部都连接到神经网络中，在图像中也是局部周边的像素联系比较紧密，而距离较远的像素则相关性较弱，因此可以采用局部连接的模式（将图像分块连接，这样能大大减少模型的参数），如下图所示：  
![](https://static.oschina.net/uploads/space/2018/0311/012842_Z2z8_876354.png)
   
**2、参数（权值）共享**  
每张自然图像（人物、山水、建筑等）都有其固有特性，也就是说，图像其中一部分的统计特性与其它部分是接近的。这也意味着这一部分学习的特征也能用在另一部分上，能使用同样的学习特征。因此，在局部连接中隐藏层的每一个神经元连接的局部图像的权值参数（例如 5×5），将这些权值参数共享给其它剩下的神经元使用，那么此时不管隐藏层有多少个神经元，需要训练的参数就是这个局部图像的权限参数（例如 5×5），也就是卷积核的大小，这样大大减少了训练参数。如下图  
![](https://static.oschina.net/uploads/space/2018/0311/012854_DfxF_876354.png)
   
**3、池化**  
随着模型网络不断加深，卷积核越来越多，要训练的参数还是很多，而且直接拿卷积核提取的特征直接训练也容易出现过拟合的现象。回想一下，之所以对图像使用卷积提取特征是因为图像具有一种 “静态性” 的属性，因此，一个很自然的想法就是对不同位置区域提取出有代表性的特征（进行聚合统计，例如最大值、平均值等），这种聚合的操作就叫做池化，池化的过程通常也被称为特征映射的过程（特征降维），如下图：  
![](https://static.oschina.net/uploads/space/2018/0311/012904_j6uj_876354.png)
   
回顾了卷积神经网络（CNN）上面的三个特点后，下面来介绍一下 CNN 的经典模型：手写字体识别模型 LeNet5。  
LeNet5 诞生于 1994 年，是最早的卷积神经网络之一， 由 Yann LeCun 完成，推动了深度学习领域的发展。在那时候，没有 GPU 帮助训练模型，甚至 CPU 的速度也很慢，因此，LeNet5 通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点，给这个领域带来了许多灵感。  
LeNet5 的网络结构示意图如下所示：  
![](https://static.oschina.net/uploads/space/2018/0311/012923_Ficx_876354.png)
   
LeNet5 由 7 层 CNN（不包含输入层）组成，上图中输入的原始图像大小是 32×32 像素，卷积层用 Ci 表示，子采样层（pooling，池化）用 Si 表示，全连接层用 Fi 表示。下面逐层介绍其作用和示意图上方的数字含义。  
**1、C1 层（卷积层）：6@28×28**  
该层使用了 6 个卷积核，每个卷积核的大小为 5×5，这样就得到了 6 个 feature map（特征图）。  
**（1）特征图大小**  
每个卷积核（5×5）与原始的输入图像（32×32）进行卷积，这样得到的 feature map（特征图）大小为（32-5+1）×（32-5+1）= 28×28  
卷积过程如下图所示：  
![](https://static.oschina.net/uploads/space/2018/0311/012939_tG24_876354.png)
   
卷积核与输入图像按卷积核大小逐个区域进行匹配计算，匹配后原始输入图像的尺寸将变小，因为边缘部分卷积核无法越出界，只能匹配一次，如上图，匹配计算后的尺寸变为 Cr×Cc=（Ir-Kr+1）×（Ic-Kc+1），其中 Cr、Cc，Ir、Ic，Kr、Kc 分别表示卷积后结果图像、输入图像、卷积核的行列大小。  
**（2）参数个数**  
由于参数（权值）共享的原因，对于同个卷积核每个神经元均使用相同的参数，因此，参数个数为（5×5+1）×6= 156，其中 5×5 为卷积核参数，1 为偏置参数  
**（3）连接数**  
卷积后的图像大小为 28×28，因此每个特征图有 28×28 个神经元，每个卷积核参数为（5×5+1）×6，因此，该层的连接数为（5×5+1）×6×28×28=122304  
**2、S2 层（下采样层，也称池化层）：6@14×14  
（1）特征图大小**  
这一层主要是做池化或者特征映射（特征降维），池化单元为 2×2，因此，6 个特征图的大小经池化后即变为 14×14。回顾本文刚开始讲到的池化操作，池化单元之间没有重叠，在池化区域内进行聚合统计后得到新的特征值，因此经 2×2 池化后，每两行两列重新算出一个特征值出来，相当于图像大小减半，因此卷积后的 28×28 图像经 2×2 池化后就变为 14×14。  
这一层的计算过程是：2×2 单元里的值相加，然后再乘以训练参数 w，再加上一个偏置参数 b（每一个特征图共享相同的 w 和 b)，然后取 sigmoid 值（S 函数：0-1 区间），作为对应的该单元的值。卷积操作与池化的示意图如下：  
![](https://static.oschina.net/uploads/space/2018/0311/012957_l7Oh_876354.png)
   
**（2）参数个数**  
S2 层由于每个特征图都共享相同的 w 和 b 这两个参数，因此需要 2×6=12 个参数  
**（3）连接数**  
下采样之后的图像大小为 14×14，因此 S2 层的每个特征图有 14×14 个神经元，每个池化单元连接数为 2×2+1（1 为偏置量），因此，该层的连接数为（2×2+1）×14×14×6 = 5880  
**3、C3 层（卷积层）：16@10×10**  
C3 层有 16 个卷积核，卷积模板大小为 5×5。  
**（1）特征图大小**  
与 C1 层的分析类似，C3 层的特征图大小为（14-5+1）×（14-5+1）= 10×10  
**（2）参数个数**  
需要注意的是，C3 与 S2 并不是全连接而是部分连接，有些是 C3 连接到 S2 三层、有些四层、甚至达到 6 层，通过这种方式提取更多特征，连接的规则如下表所示：  
![](https://static.oschina.net/uploads/space/2018/0311/013017_pIe9_876354.png)
   
例如第一列表示 C3 层的第 0 个特征图（feature map）只跟 S2 层的第 0、1 和 2 这三个 feature maps 相连接，计算过程为：用 3 个卷积模板分别与 S2 层的 3 个 feature maps 进行卷积，然后将卷积的结果相加求和，再加上一个偏置，再取 sigmoid 得出卷积后对应的 feature map 了。其它列也是类似（有些是 3 个卷积模板，有些是 4 个，有些是 6 个）。因此，C3 层的参数数目为（5×5×3+1）×6 +（5×5×4+1）×9 +5×5×6+1 = 1516  
**（3）连接数**  
卷积后的特征图大小为 10×10，参数数量为 1516，因此连接数为 1516×10×10= 151600  
**4、S4（下采样层，也称池化层）：16@5×5  
（1）特征图大小**  
与 S2 的分析类似，池化单元大小为 2×2，因此，该层与 C3 一样共有 16 个特征图，每个特征图的大小为 5×5。  
**（2）参数数量**  
与 S2 的计算类似，所需要参数个数为 16×2 = 32  
**（3）连接数**  
连接数为（2×2+1）×5×5×16 = 2000  
**5、C5 层（卷积层）：120  
（1）特征图大小**  
该层有 120 个卷积核，每个卷积核的大小仍为 5×5，因此有 120 个特征图。由于 S4 层的大小为 5×5，而该层的卷积核大小也是 5×5，因此特征图大小为（5-5+1）×（5-5+1）= 1×1。这样该层就刚好变成了全连接，这只是巧合，如果原始输入的图像比较大，则该层就不是全连接了。  
**（2）参数个数**  
与前面的分析类似，本层的参数数目为 120×（5×5×16+1） = 48120  
**（3）连接数**  
由于该层的特征图大小刚好为 1×1，因此连接数为 48120×1×1=48120  
**6、F6 层（全连接层）：84  
（1）特征图大小**  
F6 层有 84 个单元，之所以选这个数字的原因是来自于输出层的设计，对应于一个 7×12 的比特图，如下图所示，-1 表示白色，1 表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。  
![](https://static.oschina.net/uploads/space/2018/0311/013047_ApKN_876354.png)
   
该层有 84 个特征图，特征图大小与 C5 一样都是 1×1，与 C5 层全连接。  
**（2）参数个数**  
由于是全连接，参数数量为（120+1）×84=10164。跟经典神经网络一样，F6 层计算输入向量和权重向量之间的点积，再加上一个偏置，然后将其传递给 sigmoid 函数得出结果。  
**（3）连接数**  
由于是全连接，连接数与参数数量一样，也是 10164。  
**7、OUTPUT 层（输出层）：10**  
Output 层也是全连接层，共有 10 个节点，分别代表数字 0 到 9。如果第 i 个节点的值为 0，则表示网络识别的结果是数字 i。  
**（1）特征图大小**  
该层采用径向基函数（RBF）的网络连接方式，假设 x 是上一层的输入，y 是 RBF 的输出，则 RBF 输出的计算方式是：  
![](https://static.oschina.net/uploads/space/2018/0311/013103_iVGG_876354.png)
   
上式中的 Wij 的值由 i 的比特图编码确定，i 从 0 到 9，j 取值从 0 到 7×12-1。RBF 输出的值越接近于 0，表示当前网络输入的识别结果与字符 i 越接近。  
**（2）参数个数**  
由于是全连接，参数个数为 84×10=840  
**（3）连接数**  
由于是全连接，连接数与参数个数一样，也是 840

通过以上介绍，已经了解了 LeNet 各层网络的结构、特征图大小、参数数量、连接数量等信息，下图是识别数字 3 的过程，可对照上面介绍各个层的功能进行一一回顾：  
![](https://static.oschina.net/uploads/space/2018/0311/013117_gXns_876354.png)
 

**墙裂建议**

Ann LeCun 在 1998 年发表了关于 LeNet 的经典论文《Gradient-Based Learning Applied to Document Recognition 》（基于梯度学习在文档识别中的应用），里面有非常详细介绍，建议阅读这篇论文，进一步巩固知识。

扫描以下二维码关注本人公众号 “大数据与人工智能 Lab”（BigdataAILab），然后回复 “**论文**” 关键字可在线阅读这篇经典论文的内容。

![](https://static.oschina.net/uploads/space/2018/0213/155533_IdYn_876354.jpg)

**推荐相关阅读**

*   [大话卷积神经网络（CNN）](https://my.oschina.net/u/876354/blog/1620906)
*   [大话循环神经网络（RNN）](https://my.oschina.net/u/876354/blog/1621839)
*   [大话深度残差网络（DRN）](https://my.oschina.net/u/876354/blog/1622896)
*   [大话深度信念网络（DBN）](https://my.oschina.net/u/876354/blog/1626639)
*   [浅说 “迁移学习”](https://my.oschina.net/u/876354/blog/1614883)
*   [什么是 “强化学习”](https://my.oschina.net/u/876354/blog/1614879)
*   [AlphaGo 算法原理浅析](https://my.oschina.net/u/876354/blog/1594849)
*   [大数据究竟有多少个 V](https://my.oschina.net/u/876354/blog/1604254)
*   [Apache Hadoop 2.8 完全分布式集群搭建超详细教程](https://my.oschina.net/u/876354/blog/993836)
*   [Apache Hive 2.1.1 安装配置超详细教程](https://my.oschina.net/u/876354/blog/1057639)
*   [Apache HBase 1.2.6 完全分布式集群搭建超详细教程](https://my.oschina.net/u/876354/blog/1163018)
*   [离线安装 Cloudera Manager 5 和 CDH5（最新版 5.13.0）超详细教程](https://my.oschina.net/u/876354/blog/1605320)