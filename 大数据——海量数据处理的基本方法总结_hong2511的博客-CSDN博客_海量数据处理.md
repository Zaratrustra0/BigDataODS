# 大数据——海量数据处理的基本方法总结_hong2511的博客-CSDN博客_海量数据处理
原文地址为： [大数据——海量数据处理的基本方法总结](http://www.itdaan.com/blog/2015/09/03/763d8a8e0c851835f54143e6690b2ee1.html)

声明： 原文引用参考 July 大神的 csdn 博客文章 => [海量处理面试题](http://blog.csdn.net/v_july_v/article/details/6685962)

所谓海量数据处理，就是数据量太大，无法在较短时间内迅速解决，无法一次性装入内存。本文在前人的基础上总结一下解决此类问题的办法。那么有什么解决办法呢?  
时间复杂度方面，我们可以采用巧妙的算法搭配合适的数据结构，如 Bloom filter/Hash/bit-map / 堆 / 数据库或倒排索引 / trie 树。空间复杂度方面，分而治之 / hash 映射。

海量数据处理的基本方法总结起来分为以下几种：

1.  分而治之 / hash 映射 + hash 统计 + 堆 / 快速 / 归并排序；
2.  双层桶划分；
3.  Bloom filter/Bitmap；
4.  Trie 树 / 数据库 / 倒排索引；
5.  外排序；
6.  分布式处理之 Hadoop/Mapreduce。

* * *

前提基础知识：  
1 byte= 8 bit。  
int 整形一般为 4 bytes 共 32 位 bit。  
2^32=4G。  
1G=2^30=10.7 亿。

* * *

**问题 1**

> 给定 a、b 两个文件，各存放 50 亿个 url，每个 url 各占 64 字节，内存限制是 4G，让你找出 a、b 文件共同的 url？

**分析**：50 亿\*64=320G 大小空间。  
**算法思想 1**：hash 分解 + 分而治之 + 归并

1.  遍历文件 a，对每个 url 根据某种 hash 规则求取 hash(url)/1024，然后根据所取得的值将 url 分别存储到 1024 个小文件（a0~a1023）中。这样每个小文件的大约为 300M。如果 hash 结果很集中使得某个文件 ai 过大，可以在对 ai 进行二级 hash(ai0~ai1024)。
2.  这样 url 就被 hash 到 1024 个不同级别的目录中。然后可以分别比较文件，a0VSb0……a1023VSb1023。求每对小文件中相同的 url 时，可以把其中一个小文件的 url 存储到 hash_map 中。然后遍历另一个小文件的每个 url，看其是否在刚才构建的 hash_map 中，如果是，那么就是共同的 url，存到文件里面就可以了。
3.  把 1024 个级别目录下相同的 url 合并起来。

**问题 2**

> 有 10 个文件，每个文件 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求你按照 query 的频度排序。  
> **解决思想 1**：hash 分解 + 分而治之 + 归并

1.  顺序读取 10 个文件 a0~a9，按照 hash(query)%10 的结果将 query 写入到另外 10 个文件（记为 b0~b9）中。这样新生成的文件每个的大小大约也 1G（假设 hash 函数是随机的）。
2.  找一台内存 2G 左右的机器，依次对用 hash_map(query, query_count) 来统计每个 query 出现的次数。利用快速 / 堆 / 归并排序按照出现次数进行排序。将排序好的 query 和对应的 query_cout 输出到文件中。这样得到了 10 个排好序的文件 c0~c9。
3.  对这 10 个文件 c0~c9 进行归并排序（内排序与外排序相结合）。每次取 c0~c9 文件的 m 个数据放到内存中，进行 10m 个数据的归并，即使把归并好的数据存到 d 结果文件中。如果 ci 对应的 m 个数据全归并完了，再从 ci 余下的数据中取 m 个数据重新加载到内存中。直到所有 ci 文件的所有数据全部归并完成。  
    **解决思想 2**： Trie 树  
    如果 query 的总量是有限的，只是重复的次数比较多而已，可能对于所有的 query，一次性就可以加入到内存了。在这种假设前提下，我们就可以采用 trie 树 / hash_map 等直接来统计每个 query 出现的次数，然后按出现次数做快速 / 堆 / 归并排序就可以了。

**问题 3**：

> 有一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限制大小是 1M。返回频数最高的 100 个词。  
> 类似问题：怎么在海量数据中找出重复次数最多的一个？

**解决思想**： hash 分解 + 分而治之 + 归并

1.  顺序读文件中，对于每个词 x，按照 hash(x)/(1024\*4) 存到 4096 个小文件中。这样每个文件大概是 250k 左右。如果其中的有的文件超过了 1M 大小，还可以按照 hash 继续往下分，直到分解得到的小文件的大小都不超过 1M。
2.  对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用 trie 树 / hash_map 等），并取出出现频率最大的 100 个词（可以用含 100 个结点的最小堆），并把 100 词及相应的频率存入文件。这样又得到了 4096 个文件。
3.  下一步就是把这 4096 个文件进行归并的过程了。（类似与归并排序）

**问题 4**

> 海量日志数据，提取出某日访问百度次数最多的那个 IP  
> **解决思想**： hash 分解 + 分而治之 + 归并

1.  把这一天访问百度的日志中的 IP 取出来，逐个写入到一个大文件中。注意到 IP 是 32 位的，最多有 2^32 个 IP。同样可以采用 hash 映射的方法，比如模 1024，把整个大文件映射为 1024 个小文件。
2.  再找出每个小文中出现频率最大的 IP（可以采用 hash_map 进行频率统计，然后再找出频率最大的几个）及相应的频率。
3.  然后再在这 1024 组最大的 IP 中，找出那个频率最大的 IP，即为所求。

**问题 5**

> 海量数据分布在 100 台电脑中，想个办法高效统计出这批数据的 TOP10。

**解决思想**： 分而治之 + 归并。  
注意 TOP10 是取最大值或最小值。如果取频率 TOP10，就应该先 hash 分解。

1.  在每台电脑上求出 TOP10，采用包含 10 个元素的堆完成（TOP10 小，用最大堆，TOP10 大，用最小堆）。比如求 TOP10 大，我们首先取前 10 个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是 TOP10 大。
2.  求出每台电脑上的 TOP10 后，然后把这 100 台电脑上的 TOP10 组合起来，共 1000 个数据，再利用上面类似的方法求出 TOP10 就可以了。

**问题 6**

> 在 2.5 亿个整数中找出不重复的整数，内存不足以容纳这 2.5 亿个整数。

**解决思路 1** ： hash 分解 + 分而治之 + 归并  
2.5 亿个 int 数据 hash 到 1024 个小文件中 a0~a1023，如果某个小文件大小还大于内存，进行多级 hash。每个小文件读进内存，找出只出现一次的数据，输出到 b0~b1023。最后数据合并即可。

**解决思路 2** ： 2-Bitmap  
如果内存够 1GB 的话，采用 2-Bitmap（每个数分配 2bit，00 表示不存在，01 表示出现一次，10 表示多次，11 无意义）进行，共需内存 2^32\*2bit=1GB 内存。然后扫描这 2.5 亿个整数，查看 Bitmap 中相对应位，如果是 00 变 01，01 变 10，10 保持不变。所描完事后，查看 bitmap，把对应位是 01 的整数输出即可。  
注意，如果是找出重复的数据，可以用 1-bitmap。第一次 bit 位由 0 变 1，第二次查询到相应 bit 位为 1 说明是重复数据，输出即可。

**问题 7**

> 一共有 N 个机器，每个机器上有 N 个数。每个机器最多存 O(N) 个数并对它们操作。如何找到 N^2 个数中的中数？

**解决思想 1** ： hash 分解 + 排序

1.  按照升序顺序把这些数字，hash 划分为 N 个范围段。假设数据范围是 2^32 的 unsigned int 类型。理论上第一台机器应该存的范围为 0~(2^32)/N，第 i 台机器存的范围是 (2^32)\*(i-1)/N~(2^32)\*i/N。hash 过程可以扫描每个机器上的 N 个数，把属于第一个区段的数放到第一个机器上，属于第二个区段的数放到第二个机器上，…，属于第 N 个区段的数放到第 N 个机器上。注意这个过程每个机器上存储的数应该是 O(N) 的。
2.  然后我们依次统计每个机器上数的个数，一次累加，直到找到第 k 个机器，在该机器上累加的数大于或等于（N^2）/2，而在第 k-1 个机器上的累加数小于（N^2）/2，并把这个数记为 x。那么我们要找的中位数在第 k 个机器中，排在第（N^2）/2-x 位。然后我们对第 k 个机器的数排序，并找出第（N^2）/2-x 个数，即为所求的中位数的复杂度是 O（N^2）的。

**解决思想 2：**  分而治之 + 归并  
先对每台机器上的数进行排序。排好序后，我们采用归并排序的思想，将这 N 个机器上的数归并起来得到最终的排序。找到第（N^2）/2 个便是所求。复杂度是 O（N^2 \* lgN^2）的。

_这里 Trie 树木、红黑树或者 hash_map 可以认为是第一部分中分而治之算法的具体实现方法之一。_

**问题 1**

> 上千万或上亿数据（有重复），统计其中出现次数最多的钱 N 个数据。

**解决思路**： 红黑树 + 堆排序

1.  如果是上千万或上亿的 int 数据，现在的机器 4G 内存可以能存下。所以考虑采用 hash_map / 搜索二叉树 / 红黑树等来进行统计重复次数。
2.  然后取出前 N 个出现次数最多的数据，可以用包含 N 个元素的最小堆找出频率最大的 N 个数据。

**问题 2**

> 1000 万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？

**解决思路**：trie 树。  
这题用 trie 树比较合适，hash_map 也应该能行。

**问题 3**

> 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前 10 个词，请给出思想，给出时间复杂度分析。

**解决思路**： trie 树 + 堆排序  
这题是考虑时间效率。  
1. 用 trie 树统计每个词出现的次数，时间复杂度是 O(n\*len)（len 表示单词的平准长度）。  
2. 然后找出出现最频繁的前 10 个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是 O(n\*lg10)。  
总的时间复杂度，是 O(n\*le) 与 O(n\*lg10) 中较大的哪一个。

**问题 4**

> 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为 1-255 字节。假设目前有一千万个记录，这些查询串的重复读比较高，虽然总数是 1 千万，但是如果去除重复和，不超过 3 百万个。一个查询串的重复度越高，说明查询它的用户越多，也就越热门。请你统计最热门的 10 个查询串，要求使用的内存不能超过 1G。

**解决思想** ： trie 树 + 堆排序  
采用 trie 树，关键字域存该查询串出现的次数，没有出现为 0。最后用 10 个元素的最小推来对出现频率进行排序。

## 3.1 BitMap

BitMap 说白了很 easy，就是通过 bit 位为 1 或 0 来标识某个状态存不存在。可进行数据的快速查找，判重，删除，一般来说适合的处理数据范围小于 8\*2^32。否则内存超过 4G，内存资源消耗有点多。  
**问题 1**

> 已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。

**解决思路**： bitmap  
8 位最多 99 999 999，需要 100M 个 bit 位，不到 12M 的内存空间。我们把 0-99 999 999 的每个数字映射到一个 Bit 位上，所以只需要 99M 个 Bit==12MBytes，这样，就用了小小的 12M 左右的内存表示了所有的 8 位数的电话

**问题 2**

> 2.5 亿个整数中找出不重复的整数的个数，内存空间不足以容纳这 2.5 亿个整数。

**解决思路**：2bit map 或者两个 bitmap。  
将 bit-map 扩展一下，用 2bit 表示一个数即可，00 表示未出现，01 表示出现一次，10 表示出现 2 次及以上，11 可以暂时不用。  
在遍历这些数的时候，如果对应位置的值是 00，则将其置为 01；如果是 01，将其置为 10；如果是 10，则保持不变。需要内存大小是 2^32/8\*2=1G 内存。  
或者我们不用 2bit 来进行表示，我们用两个 bit-map 即可模拟实现这个 2bit-map，都是一样的道理。

## 3.2 Bloom filter

Bloom filter 可以看做是对 bit-map 的扩展。  
参考 july 大神 csdn 文章 [Bloom Filter 详解](http://blog.csdn.net/v_july_v/article/details/6685894)

参考引用 july 大神 csdn 文章  
[MapReduce 的初步理解](http://blog.csdn.net/v_july_v/article/details/6637014)  
[Hadoop 框架与 MapReduce 模式](http://blog.csdn.net/v_july_v/article/details/6704077)

转载请注明本文地址： [大数据——海量数据处理的基本方法总结](http://www.itdaan.com/blog/2015/09/03/763d8a8e0c851835f54143e6690b2ee1.html) 
 [https://blog.csdn.net/hong2511/article/details/80842704?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162717785616780265486081%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=162717785616780265486081&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-1-80842704.pc_search_result_before_js&utm_term=%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94%E6%B5%B7%E9%87%8F&spm=1018.2226.3001.4187](https://blog.csdn.net/hong2511/article/details/80842704?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162717785616780265486081%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=162717785616780265486081&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-1-80842704.pc_search_result_before_js&utm_term=%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94%E6%B5%B7%E9%87%8F&spm=1018.2226.3001.4187)
