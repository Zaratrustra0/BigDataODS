# 图解机器学习 | KNN算法及其应用
引言
--

K近邻算法（K-nearest neighbors，KNN ，有些地方也译作「K近邻算法」)是一种很基本朴实的机器学习方法。

KNN 在我们日常生活中也有类似的思想应用，比如，我们判断一个人的人品，往往只需要观察他最密切的几个人的人品好坏就能得到结果了。这就是 KNN 的思想应用，KNN 方法既可以做分类，也可以做回归。在本篇内容中，我们来给大家展开讲解 KNN 相关的知识原理。

（本篇 KNN 部分内容涉及到机器学习基础知识，没有先序知识储备的宝宝可以查看ShowMeAI的文章 [图解机器学习 | **机器学习基础知识**](https://www.showmeai.tech/article-detail/185)。

1.机器学习与分类问题
-----------

### 1）分类问题

**分类问题是机器学习非常重要的一个组成部分，它的目标是根据已知样本的某些特征，判断一个样本属于哪个类别**。分类问题可以细分如下：

![](https://img-blog.csdnimg.cn/img_convert/df822670920345e2ccf76a6e68db3df0.png)

*   **二分类问题**：表示分类任务中有两个类别新的样本属于哪种已知的样本类。
    
*   **多类分类**（Multiclass classification）问题：表示分类任务中有多类别。
    
*   **多标签分类**（Multilabel classification）问题：给每个样本一系列的目标标签。
    

### 2）分类问题的数学抽象

**从算法的角度解决一个分类问题，我们的训练数据会被映射成 *n* 维空间的样本点（这里的 *n* 就是特征维度），我们需要做的事情是对 *n* 维样本空间的点进行类别区分，某些点会归属到某个类别**。

下图所示的是二维平面中的两类样本点，我们的模型（分类器）在学习一种区分不同类别的方法，比如这里是使用一条直线去对两类不同的样本点进行切分。

![](https://img-blog.csdnimg.cn/img_convert/ae00c159398a7bcf9a02aed46273bc69.png)

常见的分类问题应用场景很多，我们选择几个进行举例说明：

*   **垃圾邮件识别**：可以作为二分类问题，将邮件分为你「垃圾邮件」或者「正常邮件」。
    
*   **图像内容识别**：因为图像的内容种类不止一个，图像内容可能是猫、狗、人等等，因此是多类分类问题。
    
*   **文本情感分析**：既可以作为二分类问题，将情感分为褒贬两种，还可以作为多类分类问题，将情感种类扩展，比如分为：十分消极、消极、积极、十分积极等。
    

2.K近邻算法核心思想
-----------

在模式识别领域中，K近邻算法（ KNN 算法，又译 *K-*最近邻算法）是一种用于分类和回归的非参数统计方法。在这两种情况下，输入包含特征空间中的 K个最接近的训练样本。

### 1）K近邻核心思想

在 KNN 分类中，输出是一个分类族群。**一个对象的分类是由其邻居的「多数表决」确定的**，K个最近邻居（ K为正整数，通常较小)中最常见的分类决定了赋予该对象的类别。

*   若 *K = 1*，则该对象的类别直接由最近的一个节点赋予。

> 在 KNN 回归中，输出是该对象的属性值。该值是其 K 个最近邻居的值的平均值。

![](https://img-blog.csdnimg.cn/img_convert/447dea20bf77dd257d0b4d430e340d49.png)

 **K近邻居法采用向量空间模型来分类，概念为相同类别的案例，彼此的相似度高**。而可以借由计算与已知类别案例之相似度，来评估未知类别案例可能的分类。

KNN 是一种基于实例的学习，或者是局部近似和将所有计算推迟到分类之后的惰性学习。 *K-*近邻算法是所有的机器学习算法中最简单的之一。

### 2）豆子分类例子

想一想：下图中只有三种豆，有三个豆的种类未知，如何判定他们的种类？

![](https://img-blog.csdnimg.cn/img_convert/59a3b1201f74e297379fdcdb93493a36.png)

1968年，Cover 和 Hart 提出了最初的近邻法，思路是——未知的豆离哪种豆最近，就认为未知豆和该豆是同一种类。

![](https://img-blog.csdnimg.cn/img_convert/0a818565c2fd37ee45f78f09f2db63b3.png)

由此，引出最近邻算法的定义：为了判定未知样本的类别，以全部训练样本作为代表点计算未知样本与所有训练样本的距离，并以最近邻者的类别作为决策未知样本类别的唯一依据。

**最近邻算法的缺陷是对噪声数据过于敏感**。从图中可以得到，一个圈起来的蓝点和两个圈起来的红点到绿点的距离是相等的，根据最近邻算法，该点的形状无法判断。

为了解决这个问题，我们可以把位置样本周边的多个最近样本计算在内，扩大参与决策的样本量，以避免个别数据直接决定决策结果。

![](https://img-blog.csdnimg.cn/img_convert/d53bc7a46f0da0789bf4c49132a0e8d3.png)

引进 *K-*近邻算法——选择未知样本一定范围内确定个数的 K个样本，该 K个样本大多数属于某一类型，则未知样本判定为该类型。*K-*近邻算法是最近邻算法的一个延伸。

> 根据 K近邻算法，离绿点最近的三个点中有两个是红点，一个是蓝点，红点的样本数量多于蓝点的样本数量，因此绿点的类别被判定为红点。

3.K近邻算法步骤与示例
------------

下面的内容首先为大家梳理下 K
 近邻算法的步骤，之后通过示例为大家展示 K
 近邻算法的计算流程。

![](https://img-blog.csdnimg.cn/img_convert/0881a71e482f90f207560ba98fc2669d.png)

### 1）K近邻算法工作原理

*   存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每个数据与所属分类的对应关系。
    
*   输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。
    
*   一般来说，只选择样本数据集中前 *n*个最相似的数据。 K一般不大于20，最后，选择 K个中出现次数最多的分类，作为新数据的分类。

### 2）K近邻算法参数选择

*   **如何选择一个最佳的 K值取决于数据**。一般情况下，在分类时较大的 K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的 K值能通过各种启发式技术（见超参数优化）来获取。
    
*   噪声和非相关性特征的存在，或特征尺度与它们的重要性不一致会使 K近邻算法的准确性严重降低。对于选取和缩放特征来改善分类已经做了很多研究。一个普遍的做法是**利用进化算法优化功能扩展**，还有一种较普遍的方法是**利用训练样本的互信息进行选择特征**。
    
*   在二元（两类）分类问题中，选取 K为奇数有助于避免两个分类平票的情形。在此问题下，选取最佳经验 K值的方法是自助法。
    

> 说明： KNN 没有显示的训练过程，它是「懒惰学习」的代表，它在训练阶段只是把数据保存下来，训练时间开销为 0，等收到测试样本后进行处理。

### 3）K近邻算法示例

举例：以电影分类作为例子，电影题材可分为爱情片，动作片等。那么爱情片有哪些特征？动作片有哪些特征呢？也就是说给定一部电影，怎么进行分类？

这里假定将电影分为爱情片和动作片两类，如果一部电影中接吻镜头很多，打斗镜头较少，显然是属于爱情片，反之为动作片。

![](https://img-blog.csdnimg.cn/img_convert/8bc14a3ef0c2765f507ffc6187f1654b.png)

有人曾根据电影中打斗动作和接吻动作数量进行评估，数据如图。给定一部电影数据 ![](https://www.zhihu.com/equation?tex=(18%2C90))
 打斗镜头 18个，接吻镜头 ![](https://www.zhihu.com/equation?tex=90)个，如何知道它是什么类型的呢？

现在我们按照距离的递增顺序排序，可以找到 K个距离最近的电影。

![](https://img-blog.csdnimg.cn/img_convert/fde965f1c1a40d3956f820227ebccf29.png)

假如 *K= 3*，那么来看排序的前 3 个电影的类别，都是爱情片，根据 KNN 的投票机制，我们判定这部电影属于爱情片。（这里的 K是超参数，可以调整，如果取 *K = 4*，那可能投票的4部电影分别是 爱情片、爱情片、爱情片、动作片，但本例中判定结果依旧为爱情片)

4.K近邻算法的缺点与改进
-------------

### 1）K近邻算法的优缺点

不同类别的样本点，分布在空间的不同区域。 K近邻是基于空间距离较近的样本类别来进行分类，本质上是对于特征空间的划分。

![](https://img-blog.csdnimg.cn/img_convert/30c992f10782a706148662ab30df02ef.png)

*   **优点**：精度高、对异常值不敏感、无数据输入假定。
*   **缺点**：计算复杂度高、空间复杂度高。
*   **适用数据范围**：数值型和标称型。

### 2）K近邻算法的核心要素：距离度量准则

近邻算法能用一种有效的方式隐含的计算决策边界。另外，它也可以显式的计算决策边界，以及有效率的这样做计算，使得计算复杂度是边界复杂度的函数。K**近邻算法依赖于空间中相近的点做类别判断，判断距离远近的度量标准非常重要**。

距离的度量标准，对很多算法来说都是核心要素（比如无监督学习的 [**聚类算法**](https://www.showmeai.tech/article-detail/197) 也很大程度依赖距离度量），也对其结果有很大的影响。

![](https://img-blog.csdnimg.cn/img_convert/1b71a3b48aedb12ef63f943ce0ad6dc1.png)

![](https://www.zhihu.com/equation?tex=Lp)**距离**（**又称闵可夫斯基距离，Minkowski Distance**)不是一种距离，而是**一组距离**的定义。

- 参数 ![](https://www.zhihu.com/equation?tex=p%3D1)时为**曼哈顿距离**（又称**L1距离**或**程式区块距离**)，表示两个点在标准坐标系上的绝对轴距之和。

*   参数 ![](https://www.zhihu.com/equation?tex=p%3D2)时为**欧氏距离**（又称**L2距离**或**欧几里得度量**)，是直线距离常见的两点之间或多点之间的距离表示法。

- 参数 ![](https://www.zhihu.com/equation?tex=p%20%5Cto%20%5Cinfty)时，就是**切比雪夫距离**（各坐标数值差的最大值)。

### 3）K近邻算法的核心要素：K的大小

对于 KNN 算法而言，K的大小取值也至关重要，如果选择较小的 K值，意味着整体模型变得复杂（模型容易发生过拟合），模型学习的近似误差（approximation error）会减小，但估计误差（estimation error）会增大。

如果选择较大的 K值，就意味着整体的模型变得简单，减少学习的估计误差，但缺点是学习的近似误差会增大。

> 在实际的应用中，一般采用一个比较小的 K 值。并采用交叉验证的方法，选取一个最优的 K 值。

### 4）K近邻算法的缺点与改进

#### （1）缺点

观察下面的例子，我们看到，对于样本*X*，通过 KNN 算法，我们显然可以得到 *X*应属于红色类别。但对于样本 *Y*，KNN 算法判定的结果是 *Y*应属于蓝色类别，然而从距离上看 *Y*和红色的批次样本点更接近。因此，原始的 KNN 算法只考虑近邻不同类别的样本数量，而忽略掉了距离。

![](https://img-blog.csdnimg.cn/img_convert/286aa9838967b91159e4516e89a49d8d.png)

除了上述缺点，KNN 还存在如下缺点：

*   **样本库容量依赖性较强对 KNN 算法在实际应用中的限制较大**：有不少类别无法提供足够的训练样本，使得 KNN 算法所需要的相对均匀的特征空间条件无法得到满足，使得识别的误差较大。
    
*   **K值的确定**： KNN 算法必须指定 K值，K值选择不当则分类精度不能保证。


#### （2）改进方法

![](https://img-blog.csdnimg.cn/img_convert/6d0c30cbad46dbfb37963dd0fd2229e6.png)

**加快 KNN 算法的分类速度**。

*   浓缩训练样本当训练样本集中样本数量较大时，为了减小计算开销，可以对训练样本集进行编辑处理，即从原始训练样本集中选择最优的参考子集进行 K近邻寻找，从而减少训练样本的存储量和提高计算效率。
    
*   加快 K个最近邻的搜索速度这类方法是通过快速搜索算法，在较短时间内找到待分类样本的 K个最近邻。
    

**对训练样本库的维护**。

*   对训练样本库进行维护以满足 KNN 算法的需要，包括对训练样本库中的样本进行添加或删除，采用适当的办法来保证空间的大小，如符合某种条件的样本可以加入数据库中，同时可以对数据库库中已有符合某种条件的样本进行删除。从而保证训练样本库中的样本提供 KNN 算法所需要的相对均匀的特征空间。

5.案例介绍
------

假如一套房子打算出租，但不知道市场价格，可以根据房子的规格（面积、房间数量、厕所数量、容纳人数等），在已有数据集中查找相似（ K 近邻）规格的房子价格，看别人的相同或相似户型租了多少钱。

![](https://img-blog.csdnimg.cn/img_convert/a8356edac6748ec4cc5bd42bc7ed7d76.png)

**分类过程**：已知的数据集中，每个已出租住房都有房间数量、厕所数量、容纳人数等字段，并有对应出租价格。将预计出租房子数据与数据集中每条记录比较计算欧式距离，取出距离最小的5条记录，将其价格取平均值，可以将其看做预计出租房子的市场平均价格。

注意：

*   最好不要将所有数据全部拿来测试，需要**分出训练集和测试集**，具体划分比例按数据集确定。
    
*   理想情况下，数据集中每个字段取值范围都相同。但实际上这是几乎不可能的，如果计算时直接用原数数据计算，则会造成较大训练误差。所以需要**对各列数据进行标准化或归一化操作，尽量减少不必要的训练误差**。
    
*   **数据集中非数值类型的字段需要转换**，替换掉美元$符号和千分位逗号。
    

机器学习【算法】系列教程
------------

*   [图解机器学习 | 机器学习基础知识](https://www.showmeai.tech/article-detail/185)
*   [图解机器学习 | 模型评估方法与准则](https://www.showmeai.tech/article-detail/186)
*   [图解机器学习 | KNN算法及其应用](https://www.showmeai.tech/article-detail/187)
*   [图解机器学习 | 逻辑回归算法详解](https://www.showmeai.tech/article-detail/188)
*   [图解机器学习 | 朴素贝叶斯算法详解](https://www.showmeai.tech/article-detail/189)
*   [图解机器学习 | 决策树模型详解](https://www.showmeai.tech/article-detail/190)
*   [图解机器学习 | 随机森林分类模型详解](https://www.showmeai.tech/article-detail/191)
*   [图解机器学习 | 回归树模型详解](https://www.showmeai.tech/article-detail/192)
*   [图解机器学习 | GBDT模型详解](https://www.showmeai.tech/article-detail/193)
*   [图解机器学习 | XGBoost模型最全解析](https://www.showmeai.tech/article-detail/194)
*   [图解机器学习 | LightGBM模型详解](https://www.showmeai.tech/article-detail/195)
*   [图解机器学习 | 支持向量机模型详解](https://www.showmeai.tech/article-detail/196)
*   [图解机器学习 | 聚类算法详解](https://www.showmeai.tech/article-detail/197)
*   [图解机器学习 | PCA降维算法详解](https://www.showmeai.tech/article-detail/198)

机器学习【实战】系列教程
------------

*   [机器学习实战 | Python机器学习算法应用实践](https://www.showmeai.tech/article-detail/201)
*   [机器学习实战 | SKLearn入门与简单应用案例](https://www.showmeai.tech/article-detail/202)
*   [机器学习实战 | SKLearn最全应用指南](https://www.showmeai.tech/article-detail/203)
*   [机器学习实战 | XGBoost建模应用详解](https://www.showmeai.tech/article-detail/204)
*   [机器学习实战 | LightGBM建模应用详解](https://www.showmeai.tech/article-detail/205)
*   [机器学习实战 | Python机器学习综合项目-电商销量预估](https://www.showmeai.tech/article-detail/206)
*   [机器学习实战 | Python机器学习综合项目-电商销量预估<进阶方案>](https://www.showmeai.tech/article-detail/207)
*   [机器学习实战 | 机器学习特征工程最全解读](https://www.showmeai.tech/article-detail/208)
*   [机器学习实战 | 自动化特征工程工具Featuretools应用](https://www.showmeai.tech/article-detail/209)
*   [机器学习实战 | AutoML自动化机器学习建模](https://www.showmeai.tech/article-detail/210)

[ShowMeAI](https://www.showmeai.tech/) 系列教程推荐
---------------------------------------------

*   [大厂技术实现：推荐与广告计算解决方案](https://www.showmeai.tech/tutorials/50)
*   [大厂技术实现：计算机视觉解决方案](https://www.showmeai.tech/tutorials/51)
*   [大厂技术实现：自然语言处理行业解决方案](https://www.showmeai.tech/tutorials/52)
*   [图解Python编程：从入门到精通系列教程](https://www.showmeai.tech/tutorials/56)
*   [图解数据分析：从入门到精通系列教程](https://www.showmeai.tech/tutorials/33)
*   [图解AI数学基础：从入门到精通系列教程](https://www.showmeai.tech/tutorials/83)
*   [图解大数据技术：从入门到精通系列教程](https://www.showmeai.tech/tutorials/84)
*   [图解机器学习算法：从入门到精通系列教程](https://www.showmeai.tech/tutorials/34)
*   [机器学习实战：手把手教你玩转机器学习系列](https://www.showmeai.tech/tutorials/41)
*   [深度学习教程：吴恩达专项课程 · 全套笔记解读](https://www.showmeai.tech/tutorials/35)
*   [自然语言处理教程：斯坦福CS224n课程 · 课程带学与全套笔记解读](https://www.showmeai.tech/tutorials/36)