# 数据开发必经之路-数据倾斜
![](https://mmbiz.qpic.cn/mmbiz_gif/7QRTvkK2qC6nNyjd9QeAUdlJnqcbr4YsiaJBYGWoeEEFUicUo1STkXfMNjmDrdbO9Jf04Q6luKiaYAyjTWMQuofCg/640?wx_fmt=gif)

点击上方蓝字  关注大数据知识

### 前言

> 数据倾斜是数据开发中最常见的问题，同时也是面试中必问的一道题。那么何为数据倾斜？什么时候会出现数据倾斜？以及如何解决呢？
>
> 何为数据倾斜：数据倾斜其本质就是**数据分配不均匀**，部分任务处理大量的数据量导致整体 job 的执行时间拉长。
>
> 什么时候出现数据倾斜：无论是 spark, 还是 mapreduce，数据倾斜大部分都是出现在 shuffle 阶段，也就是所谓的洗牌，由于使用的洗牌策略不一样，那么数据划分也就不一样，一般常用的也就是 hash 算法了。
>
> 基于上面两个问题的解答，对于数据倾斜的解决方案**其本质就是如何把数据分配均匀**。
>
> 笔者认为根据优化策略可以分为**业务层面的优化**和**技术层面的优化**
>
> 首先业务层面的优化就是结合实际的业务场景和数据特性进行优化，而技术层面的优化其本质就是对存储和计算两大组件的优化，然后根据不同的技术 (hive,spark) 使用不同的参数或者函数方法。
>
> 接下来将分别对这两种策略进行详细讲解

### 技术层面优化

#### Hive

> 这里讨论的是仍然是以 mapreduce 为底层引擎，hive on tez 这种模式不做讲解，其优化思想还有参数大多数都是一样的。

##### 1. 参数优化

> 这里给出一些关于数据倾斜相关的参数配置，一般只能起到缓解的作用，不能完全解决倾斜问题。其中有些优化参数暂时未涉及 (如 map 端，reduce 端, jvm, 压缩等有优化点)

| 参数                                 | 参数值      | 描述                                                                                                                                                                                                               |
| ---------------------------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| hive.map.aggr                      | true     | map 端聚合, 相当于 Combiner, 其思想主要是分发到 reduce 的数据量减少                                                                                                                                                                   |
| hive.groupby.skewindata            | true     | 设置为 true 时会生成两个 mr job. 在第一个 job 中, Map 端输出的结果会随机分布到 reduce 中, 每个 reduce 做部分聚合, 并返回结果, 其目的是将相同的 key 有可能分发到不同的 reduce 中, 起到负载均衡. 第二个 mr job 再根据预处理的结果按照 key 分布到 reduce 中 (这一步的目的是可以保证相同的 key 最终会被分配到同一个 reduce 中) |
| hive.auto.convert.join             | true     | 是否将 common join（reduce 端 join）转换成 map join                                                                                                                                                                       |
| hive.mapjoin.smalltable.filesize   | 25000000 | 判断为小表的输入文件大小阈值，默认 25M                                                                                                                                                                                            |
| hive.groupby.mapaggr.checkinterval | 100000   | 在 Map 端进行聚合操作的条目数目                                                                                                                                                                                               |
| hive.mapjoin.cache.numrows         | 25000    | 缓存 build table 的多少行数据到内存                                                                                                                                                                                         |
| hive.optimize.skewjoin             | true     | 当开启该选项时, 在 join 过程中 Hive 会将计数超过阈值 hive.skewjoin.key（默认 100000）的倾斜 key 对应的行临时写进文件中，然后再启动另一个 job 做 map join 生成结果                                                                                                   |
| hive.merge.mapfiles                | true     | 合并小文件，减少对应的 map 数                                                                                                                                                                                                |
| hive.skewjoin.key                  | 100000   | 判断数据倾斜的阈值，如果在 join 中发现同样的 key 超过该值，则认为是该 key 是倾斜 key                                                                                                                                                             |

##### 2.Sql 优化

> 数据倾斜出现的原因一方面是数据特性，另一方面是人为导致的, sql 开发较粗糙导致的 (占主要部分)。这里给出几个常见的 sql 倾斜的场景以及解决思路

##### 2.1 Join 优化

> hive 根据 join key 进行划分并发生 shuffle，所以选用的 key 尽量分布均匀。目前出现的场景无非就是大表和小表关联，小表和小表关联 (一般不会出现倾斜)，大表和大表关联，这里针对这几种情况分别进行讲解

###### 2.1.1 大表 join 小表 - MapJoin

> 这里的大小表是相对来说的，如果一个 A 表数据量有 1 亿，而 B 表有 1 千万，那么 B 表就是一个小表。当 B 表中的 key 分布比较集中，那么在进行 shuffle 的时候就会有一个或者某几个 reduce 上的数据量较高于平均值, 也就更容易出现倾斜。针对这种场景一般通过 mapjoin 的方式来解决。
>
> **MapJoin**的原理就是把小表全部加载到内存中（注意如果内存无法存储 1 千万的数据，需要对内存进行调节），在 map 端进行 join，这样就不会有 shuffle 阶段了。

     1--原始sql 2select  3  lnc.request_url, 4  count(uuid) as pv 5from wedw_dwd.log_ng_channel lnc 6join  7( 8  select  9     request_url,10     visit_time,11     uuid12  from wedw_dwd.track_beacon_log13)t14on lnc.request_url = t.request_url15group by lnc.request_url1617--mapjoin 18select /*+ MAPJOIN(lnc) */  19  lnc.request_url,20  count(uuid) as pv21from wedw_dwd.log_ng_channel lnc22join 23(24  select 25     request_url,26     visit_time,27     uuid28  from wedw_dwd.track_beacon_log29)t30on lnc.request_url = t.request_url31group by lnc.request_url

###### 2.1.2 大表 join 大表 - Skewjoin

> 当两个表都非常大，无法直接加载到内存中, 那么这个时候就需要评估 join key 的分布是否均匀。
>
> 情况一：当 key 分布均匀，那么这个时候一般就不是倾斜的范畴了，需要考虑增加 reduce 数量等其他调优手段了
>
> 情况二：当 key 分布不均匀，如果只是某几个 key 数据量比较大，那么就需要把这几个 key 单独拿出来进行计算；如果大部分 key 的数据量都很大，那么这个时候就需要进行增加随机前缀的方式了，也就是二次聚合的思想。

      1--参数调节  2set hive.optimize.skewjoin=true;  3set hive.skewjoin.key=100000;  4set hive.optimize.skewjoin.compiletime=true;  5  6-- 某几个key的数据量比较大,需要单独进行计算  7select   8  lnc.request_url,  9  count(uuid) as pv 10from wedw_dwd.log_ng_channel lnc 11join  12( 13  select  14     request_url, 15     visit_time, 16     uuid 17  from wedw_dwd.track_beacon_log 18  where request_url!='www.baidu.com' 19)t 20on lnc.request_url = t.request_url 21group by lnc.request_url 22 23union all  24 25select  26  lnc.request_url, 27  count(uuid) as pv 28from wedw_dwd.log_ng_channel lnc 29join  30( 31  select  32     request_url, 33     visit_time, 34     uuid 35  from wedw_dwd.track_beacon_log 36  where request_url='www.baidu.com' 37)t 38on lnc.request_url = t.request_url 39group by lnc.request_url 40 41--大部分key的数据量都比较大，采用随机前缀的方式,右表的数据量同样也需要进行扩充 42select  43  split(request_url,'&')[1] as request_url, 44  sum(cnt) as cnt 45from  46( 47  select  48    t1.request_url, 49    count(uuid) as cnt 50  from  51  ( 52    select  53       concat(cast(round(rand()*10) as int),'&',request_url) as request_url 54    from wedw_dwd.log_ng_channel 55  )t1 56  left join  57  (  -- 扩充10倍 58    select  59      concat('1&',request_url) as request_url, 60      uuid 61    from wedw_dwd.track_beacon_log 62    union all  63    select  64      concat('2&',request_url) as request_url, 65      uuid 66    from wedw_dwd.track_beacon_log 67    union all  68    select  69      concat('3&',request_url) as request_url, 70      uuid 71    from wedw_dwd.track_beacon_log 72    union all  73    select  74      concat('4&',request_url) as request_url, 75      uuid 76    from wedw_dwd.track_beacon_log 77    union all  78    select  79      concat('5&',request_url) as request_url, 80      uuid 81    from wedw_dwd.track_beacon_log 82    union all  83    select  84      concat('6&',request_url) as request_url, 85      uuid 86    from wedw_dwd.track_beacon_log 87    union all  88    select  89      concat('7&',request_url) as request_url, 90      uuid 91    from wedw_dwd.track_beacon_log 92    union all  93    select  94      concat('8&',request_url) as request_url, 95      uuid 96    from wedw_dwd.track_beacon_log 97    union all  98    select  99      concat('9&',request_url) as request_url,100      uuid101    from wedw_dwd.track_beacon_log102    union all 103    select 104      concat('10&',request_url) as request_url,105      uuid106    from wedw_dwd.track_beacon_log107  )t2108  on t1.request_url = t2.request_url109  group by t1.request_url110)t111group by split(request_url,'&')[1]

##### 2.2 distinct 优化

> distinct 操作也会经历 shuffle 阶段，通常会和 group by 进行结合使用，也是数据倾斜的高频操作。通常对于需要 distinct 的操作，我们可以换种思路来解决，即先进行 group by 后再进行后续的操作。例子如下:

     1--原始sql 2select  3   request_url, 4   count(distinct uuid) 5from wedw_dwd.log_ng_channel 6group by request_url 7 8--上面的sql可以改写为 9select 10  request_url,11  sum(1) as cnt12from 13(14  select 15  request_url16  ,uuid17from wedw_dwd.log_ng_channel18group by request_url,uuid19)t20group by request_url

##### 2.3 过滤 / 拆分

> **过滤：** 通常在进行统计的时候，表中总会有很多脏数据或者空数据，当实际的需求中并不关心这些脏数据或者空数据，那么我们可以先进行过滤，然后进行后续的操作。通过减少数据量来避免数据倾斜
>
> **拆分：** 这里和上面讲述到的 SkewJoin 优化思想很类似。
>
> 情况一：例如表中有很多 NULL 值，在整个 key 分布中占比最高，但是实际需求还不能对这些 null 值进行过滤，那么对需要单独把这些 null 值拿出来计算，或者以随机数进行填充
>
> 情况二：例如当表中的大部分 key 占比都比较高，那么这个时候就需要对这些 key 增加随机前缀，使得 reduce 分布均匀

     1--过滤 2select  3  request_url, 4  count(1) as cnt 5from wedw_dwd.log_ng_channel 6where request_url is not null and length(request_url)>0 and to_date(visit_time)>='2020-10-01' 7group by request_url 8 9--拆分10select 11  request_url,12  count(1) as cnt13from wedw_dwd.log_ng_channel14where request_url is not null and length(request_url)>015group by request_url16union all 17select 18  request_url,19  count(1) as cnt20from wedw_dwd.log_ng_channel21where request_url is  null 22group by request_url

#### Spark

> 针对 spark，当某一个或几个 task 处理时间较长且处理数据量很大，那么就是倾斜的问题了，对于 spark 的数据倾斜，和 Hive 的解决思路是一样的，但是 Hive 通常是以 sql 为主，而 Spark 是对 rdd 的操作，所以优化细节还是有些区别的。其实无论是 spark 还是 hive，数据倾斜的问题无非就是数据特性 (本身分布就不均匀 / 数据量本身就比较大) 或者是后续人为开发编写逻辑导致的。

##### 1. 排查数据源

> 在 spark 中，Stage 的划分是通过 shuffle 算子为界限的，同一个 Stage 的不同 partition 可以并行处理，不同 Stage 之间只能串行处理。一个 Stage 的整体耗时是由最慢的 task 来决定的，针对同一个 Stage 内的不同 task，排除每个计算能力差异的前提下，处理时长是由每个 task 所处理的数据量来决定的，而 Stage 的数据源主要分为两大类：
>
> 1.  获取上一个 stage 的 shuffle 数据
> 2.  直接对接数据源，如 kafka,hdfs,local filesystem
>
> 如果**对接 kafka**，则需要结合 kafka 监控来排查分区数据分布是否均匀，如果某一个分区的消息比其他分区都要多，那么这个时候就要对分区分配策略进行调整；或者分区数比较少，则需要增大 partition 数量；
>
> 如果**对接 hdfs 且不可分割的文件**，每个文件对应一个 partition, 这个时候就要看每个文件的数据量是否比较均匀了 (注意这里不能仅看文件大小，如果是压缩文件, 需要看数据量)。
>
> 对于**hdfs 可切分文件**，每个 split 大小由以下算法决定。其中 goalSize 等于所有文件总大小除以 minPartitions。而 blockSize，如果是 HDFS 文件，由文件本身的 block 大小决定；如果是 Linux 本地文件，且使用本地模式，由`fs.local.block.size`决定。
>
>     1protected long computeSplitSize(long goalSize, long minSize, long blockSize) {2    return Math.max(minSize, Math.min(goalSize, blockSize));3}
>
> 一般情况下每个 split 的大小相当于一个 block 的大小，通常不会出现倾斜。如果有的话则调整对于的参数即可。
>
> 接下来对含有 shuffle 阶段的现象进行排查定位。
>
> **总结：尽量使用可切分的文件，源头增加并行度，避免倾斜**

##### 2. 定位导致倾斜的代码

> 根据第一步排查数据源分布，如果是均匀的，那么出现倾斜就有可能是人为开发导致的，这时候就需要定位到具体代码导致的数据倾斜 (一般找发生 shuffle 算子, 如 distinct,groupByKey,join,repartition,reduceByKey,cogroup 等算子)

###### 2.1. 查看 task 运行时间和处理数据量

> 这里可以通过 spark web ui 界面来查看每个 task 的处理时长和处理的数据量

![](https://mmbiz.qpic.cn/sz_mmbiz_png/fBHtXf4CBicIJyML860m3SKQl5GGJOhoCQiadPTQ2xnjAGPuBYJqbwN4Uw3CU2f9Pwqs9WtuU6pSrXewy4XCpHYg/640?wx_fmt=png)

###### 2.2. 推断倾斜代码

> 基于 spark web ui 查看到每个 task 的处理情况，那么可以查看到该 task 是处于哪个 Stage 阶段，然后在代码中查找会出现 shuffle 的算子，以此来定位具体是哪段代码引起的数据倾斜。这里给出 wordcount 的例子来简单说明一下

    1val conf = new SparkConf()2val sc = new SparkContext(conf)3val lines = sc.textFile("hdfs://project/log/test/word.txt")4val words = lines.flatMap(_.split(","))5val pairs = words.map((_, 1))6val wordCounts = pairs.reduceByKey(_ + _)7wordCounts.collect().foreach(println(_))

> 通过上面的代码可以看到只有 reduceByKey 是会经历 shuffle 阶段的，因此只有这里才会有倾斜的可能

##### 3. 解决倾斜

> 查阅各种资料，网上列举出了 8 种解决方案，笔者这里进行了分类汇总, 尽量把每种方案都融合进来了

###### 3.1 调整并行度

> Spark Shuffle 默认使用的是 HashPartitioner 进行数据分区，当执行 shuffle read 的时候，根据**spark.sql.shuffle.partitions**参数来决定 read task 数量，默认值是 200. 当有大量的不同 key 被分配到同一个 task 的时候，可能会导致该 Task 所处理的数据远大于其他 task.
>
> 因此调整并行度的本质就是将使得原本被分配到同一 Task 的不同 Key 发配到不同 Task 上处理，则可降低原 Task 所需处理的数据量，起到缓解倾斜的作用。具体的调整方式可以通过上述参数指定并行度，或者在使用 shuffle 算子的时候指定参数值。
>
> **注意：这种优化方式对同一个 key 有大数据量的场景不适用，且只能起到缓解倾斜的作用**
>
> 如：

     1SparkSession sparkSession = SparkSession.builder() 2                .appName("wordcount") 3                .master("local") 4                .getOrCreate(); 5 6JavaSparkContext javaSparkContext = JavaSparkContext.fromSparkContext(sparkSession.sparkContext()); 7JavaRDD<String> javaRDD = javaSparkContext.textFile("hdfs://bigdatatest-1:8020/tmp/test.txt"); 8javaRDD.mapToPair(x -> new Tuple2<>(x.toLowerCase() , 1)) 9                .reduceByKey((a, b) -> a + b,1000) /**这里的1000就是并行度partition的设置*/10                .collect()11                .forEach(new Consumer<Tuple2<String, Integer>>() {12                    @Override13                    public void accept(Tuple2<String, Integer> stringIntegerTuple2) {14                        System.out.println("The Key:"+stringIntegerTuple2._1+" Value:"+stringIntegerTuple2._2.toString());15                    }

![](https://mmbiz.qpic.cn/sz_mmbiz_png/fBHtXf4CBicIJyML860m3SKQl5GGJOhoCia4lkUx6fHDYIM0HCP9icBxtUHty12nsOdK91mJ93arXGiccCFWXBpH9A/640?wx_fmt=png)

###### 3.2 自定义 partitioner

> 根据上面提到的，spark 默认使用的是 hash partition，有些时候调整并行度仍然不能有效解决数据倾斜问题，那么这个时候就需要结合实际的数据特性来自定义分区. 其主要目的还是将不同的 key 尽量分配到不同的 task 上，**这种方式对同一个 key 有大数据量的场景并不适用**

     1/** 2*自定义partition 3*/ 4public class DefinePartition extends Partitioner { 5    public  DefinePartition(){} 6 7    @Override 8    public int hashCode() { 9        return super.hashCode();10    }1112    @Override13    public boolean equals(Object definePartition) {14        DefinePartition definePartition1 = (DefinePartition) definePartition;15        return this.numPartitions()==((DefinePartition) definePartition).numPartitions();16    }1718    @Override19    public int numPartitions() {20        return 20;21    }2223    @Override24    public int getPartition(Object key) {25        int Code  = 0;26        try {27            String host = new URL(key.toString()).getHost();28            Code = host.hashCode()%numPartitions();29            if(Code<0){30                Code+=numPartitions();31            }32        } catch (MalformedURLException e) {33            e.printStackTrace();34        }3536        return Code;37    }38}

###### 3.3 过滤数据

> 这里过滤数据有两种说法：
>
> 1.  过滤无用数据：这里无用数据指的是针对特定的业务需求场景来说的，如空数据，对本次需求无影响的数据。因此在进行 shuffle 前可调用 filter 算子来过滤掉
> 2.  过滤掉少部分数据量较多的 key: 这里所说的过滤并不是真正的过滤掉，而是通过抽样的方式统计出哪些 key 所占有的数据量较多，先提前提取出来进行单独计算。和 hive 的处理思想是一样的
>
> **注意：该种方式仅对于少部分 key 有倾斜的现象有效**

###### 3.4 避免使用 shuffle 类算子

> 相信大部分读者都知道在 shuffle 过程中会把多个节点上同一个 key 拉取到同一个节点上进行计算，这个时候就会涉及到磁盘 io, 网络传输，这也就是为什么 shuffle 效率较差的原因了。
>
> 因此在实际开发中，尽量规避使用 shuffle 类算子。例如不使用 join 算子，而是使用 broadcast+map 的方式来实现。

     1 List<Map<String, String>> collect = javaSparkContext.textFile("hdfs://bigdatatest-1:8020/tmp/test.txt") 2                .mapPartitions(new FlatMapFunction<Iterator<String>, Map<String, String>>() { 3                    @Override 4                    public Iterator<Map<String, String>> call(Iterator<String> stringIterator) throws Exception { 5                        List<Map<String, String>> list = new ArrayList<>(); 6                        HashMap<String, String> hashMap = Maps.newHashMap(); 7                        while (stringIterator.hasNext()) { 8                            String str = stringIterator.next(); 9                            hashMap.put(str.split(",")[0], str.split(",")[1]);10                        }11                        list.add(hashMap);12                        return list.iterator();13                    }14                }).collect();1516Broadcast<List<Map<String, String>>> listBroadcast = javaSparkContext.broadcast(collect);17javaRDD.map(new Function<String, String>() {18  @Override19  public String call(String s) throws Exception {20    Iterator<Map<String, String>> iterator = listBroadcast.getValue().iterator();21    while (iterator.hasNext()) {22      Map<String, String> stringMap = iterator.next();23      if (stringMap.containsKey(s)) {24        return stringMap.get(s);25      }26    }27    return null;28  }29}).collect();

###### 3.5 加盐操作 (二次聚合 / 随机前缀 + 扩容)

> 笔者认为二次聚合的手段和随机前缀 + 扩容的方式其本质都是加盐操作，即对 key 进行加盐使得分配到不同的 task 上，然后再进行合并保证同一个 key 最终会聚合到一起。虽然两者思想一样，但是使用的场景还是有所区别的。
>
> **二阶段聚合：** 
>
> 1.  第一次是局部聚合，先给每个 key 打上随机数
> 2.  然后执行聚合操作, 如 reduceByKey,groupByKey, 这个时候得到的结果相对原始数据集肯定是少了很多
> 3.  然后再把 key 上的随机数给删除，保证原始数据集中的相同 key 可以被分配到同一个 task 上
> 4.  再次进行聚合操作，得到最终结果
>
> **注意：二次聚合仅适用于聚合类的 shuffle 操作**

![](https://mmbiz.qpic.cn/sz_mmbiz_png/fBHtXf4CBicIJyML860m3SKQl5GGJOhoC6bWCX3IymCib9tEdhciaN0H1nXLibfNZ7XewAa2RlUVPLvWvnHxZ1ac9w/640?wx_fmt=png)

> **随机前缀 + 扩容**
>
> 该种优化手段可以参考前面 hive 的解决方案，其思想都是一样的。
>
> **注意：和二次聚合的场景不一致，这里是针对 join 类型的操作; 无论是少数 key 倾斜还是大部分 key 倾斜都适用，但是需要对 rdd 进行扩容，需要均衡考虑内存资源**

###### 3.6 各种 join 转换

> 首先我们先来简单了解下 spark 的几种 join 实现，已经适用的场景。
>
> 1.  **Broadcast Hash Join:** **适合一张积小的表和一张大表进行 join**，其原理将其中一张小表广播分发到另一张大表所在的分区节点上，分别并发地与其上的分区记录进行 hash join。broadcast 适用于小表很小，可以直接广播的场景，当然被广播的表需要小于**spark.sql.autoBroadcastJoinThreshold**所配置的值，默认是 10M，如果增加该参数值，请考虑 driver 端的内存避免 oom，因为一般被广播的表需要 collect 到 driver 端，然后再分发到 executor 端。
> 2.  **Shuffle Hash Join:** **适合一张小表和一张大表进行 join, 或者是两张小表之间的 join**，这里所说的小表要比 broadcast hash join 场景下的小表大些，且不适合 broadcast 方式。该 join 的原理是利用 key 相同必然分区相同的这个原理，**两个表中，key 相同的行都会被 shuffle 到同一个分区中，**SparkSQL 将较大表的 join 分而治之，先将表划分成 n 个分区，再对两个表中相对应分区的数据分别进行 Hash Join，这样即在一定程度上减少了 driver 广播一侧表的压力，也减少了 executor 端取整张被广播表的内存消耗
> 3.  **Sort Merge Join:** **适合两张较大的表之间进行 join**。其原理首先将两张表按照 join key 进行了重新 shuffle，保证 join key 值相同的记录会被分在相应的分区，分区后对每个分区内的数据进行排序，排序后再对相应的分区内的记录进行连接，因为两个序列都是有序的，从头遍历，碰到 key 相同的就输出；如果不同，左边小就继续取左边，反之取右边。其原则就是**即用即取即丢**
>
> 目前网上大部分资料都是针对 reduce join 转换为 map join 的解决方案，其原理就是上面第一种 join 方式，即 broadcast+map 的方式，笔者认为第二种 join 方式也就是对应的上面的加盐操作。基于上面三种 join 方式的简单讲述，读者可根据实际内存资源、带宽资源适量将参数**spark.sql.autoBroadcastJoinThreshold**调大，让更多 join 实际执行为 broadcast hash join。

![](https://mmbiz.qpic.cn/sz_mmbiz_png/fBHtXf4CBicIJyML860m3SKQl5GGJOhoCVSse92QSKmsakYRIGcibHBpuvOSM9ZdyXZcyqjeO6jS069y01ia0c2Nw/640?wx_fmt=png)

### 业务层面优化

> 通过上面的技术层面的倾斜解决思路讲解，其实已经把业务层面的相关优化给了出来。
>
> **结合实际业务需求过滤无用数据**，尽量规避 shuffle 的发生。
>
> **逆向统计**：比如说需求方想要统计某个页面的跳出率，正常逻辑的统计是: 该页面下一页为空的 UV / 该页面 UV，那么我们可以通过 1-（该页面下一页不为空的 UV / 该页面 UV）来得到跳出率，即所谓的逆向推导
>
> **打回需求：** 当然如果读者认为该需求不切合实际，且产出意义不大，是完全可以打回的，不过笔者给出的这种优化实属是下下策，作为一名技术人员，我们还是应该正向面对，彻底解决问题的

### 总结

> 如果读者能读到这里，首先非常感谢你的耐心阅读，本文基本上都是给出解决思路，实际案例较少。因为在实际场景中，可能并不是该文中的某一个方案能够彻底解决读者的问题，有时候需要结合多种其他的优化方式结合倾斜的解决思路才能够解决，笔者认为掌握一个问题的解决思路才是重要的。如果读者比较细心的话，可能会发现本文中出现比较多的字就是**均匀**，因此倾斜对应的反义词就是均匀，这也就是解决数据倾斜的主线，即一切解决方案都是围绕着均衡来展开的。

![](https://mmbiz.qpic.cn/mmbiz_gif/dXCnejTRMLfaPKxaibsZ7cCiaozWvibvo25R8yoqsvvTmiaG2PAMap0daZ8F31icEtXsianE5bA67SQ5Lh1fAbT9yLvA/640?wx_fmt=gif)

[元数据管理 - 技术元数据解决方案](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247484191&idx=1&sn=1b783887eafb3abbde161d4f18941945&chksm=fd4e1cfbca3995ed7b77cc1ea205a65e3b793208fe09da557f98bfb2d8d2d3f83bfce497f50c&scene=21#wechat_redirect)  

[一万字完整总结 Flume](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247484174&idx=1&sn=1bfb34e54444a3a5191fcf3cb5c10ac0&chksm=fd4e1ceaca3995fc4d0256283f794d4a06b98f362ae57183f80b6ab305040356c4f22ce1071d&scene=21#wechat_redirect)  
[SparkStreaming 完整学习教程](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247484129&idx=1&sn=7ef6bd2aaff5e3362a64033104dbd282&chksm=fd4e1d05ca399413263233eb72f83dbc8b54aa16fcdaffa635d3c1493a41c2d476bf41cdc758&scene=21#wechat_redirect)  

[数仓利器 - Hive 高频函数合集](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247484127&idx=1&sn=2674ac8279992c6e6de05cadecaa6ac4&chksm=fd4e1d3bca39942da8e3537735160a5f62115a0a5b620db0416714ff25dec1cfdd00ed8ff629&scene=21#wechat_redirect)  

[2020 年大厂面试题 - 数据仓库篇](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247484057&idx=1&sn=9df87f8a364825246625242df5b41e00&chksm=fd4e1d7dca39946b66e25a83a13a3ae662101e9fcb5c556a25fcace3228a811088021bf09d2f&scene=21#wechat_redirect)  

[zookeeper 源码解读之 - DataTree 模型构建 + Leader 选举](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247484002&idx=1&sn=aa366400c6a344bef86e25e40892c0cb&chksm=fd4e1d86ca39949040611fc4f2d6e3b0ea54d3f7019a2e727a70c9257fca7509edc1628fbacb&scene=21#wechat_redirect)  

[zookeeper 源码解读之 - 服务端启动流程](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247484001&idx=1&sn=343bdeae32355d665d2097ebfbe27074&chksm=fd4e1d85ca39949376bb8b12d360095967b1258dd6e3a6477c5729148bb35e52e91ed710fe47&scene=21#wechat_redirect)  

[一段 SQL 简述鬼谷子问徒问题](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247483933&idx=1&sn=2f639f8882a3c7f3e954e24d9e41e7c0&chksm=fd4e1df9ca3994efe3a44e0b2342fe9d25943fe23b5c3a81f2ad1a50b8f0a261c250fd8e2ee4&scene=21#wechat_redirect)  

[实战：如何实时采集上亿级别数据？](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247483853&idx=1&sn=ea94cf27163368305972ca9ff996feb9&chksm=fd4e1e29ca39973f5ce24f800ec3512d353e6af205e6fb2ecacfb2603ee927bafdb6a0d06266&scene=21#wechat_redirect)  

[Spark 数据倾斜之骚操作解决方案](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247483826&idx=1&sn=8164022d80a8317cc374204a0840392d&chksm=fd4e1e56ca399740cba4e486c331db5926cdb269f93bff2d0460ed1b91192034db94c4cca3bb&scene=21#wechat_redirect)  

[0006-01-01 Kafka 深度剖析 HW 以及 LEO](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247483814&idx=1&sn=e31e1ca19952bffce5c740aa403ab3d0&chksm=fd4e1e42ca399754ec74dc55ed492252ba86c22bc6bbc4c7ef6653ba3a58aba4188da668c70c&scene=21#wechat_redirect)  

[0004-01-03 Livy REST 提交 Spark 作业](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247483785&idx=1&sn=4d40f3178d2ece4b985cbefac1401167&chksm=fd4e1e6dca39977bc82d399ebbeff133361fe88a4a917b5758e2c531b4e26fcee051534e02ce&scene=21#wechat_redirect)  

[数据同步神器 - Datax 源码重构](http://mp.weixin.qq.com/s?__biz=MzU4MDkzMDE4Nw==&mid=2247484079&idx=1&sn=6793e2912a69cb0a402d8ea7421a74f6&chksm=fd4e1d4bca39945da6f5b391b3cfd1852af3509660a87a982c9502faab4a0eee4493616c52de&scene=21#wechat_redirect) 
 [https://mp.weixin.qq.com/s/CZd2QjF4M8SACly3HJKX3w](https://mp.weixin.qq.com/s/CZd2QjF4M8SACly3HJKX3w)
